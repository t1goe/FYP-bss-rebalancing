{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominican-constitution",
   "metadata": {},
   "source": [
    "# LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mobile-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "# def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "#     n_vars = 1 if type(data) is list else data.shape[1]\n",
    "#     df = DataFrame(data)\n",
    "#     cols, names = list(), list()\n",
    "#     # input sequence (t-n, ... t-1)\n",
    "#     for i in range(n_in, 0, -1):\n",
    "#         cols.append(df.shift(i))\n",
    "#         names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "#     # forecast sequence (t, t+1, ... t+n)\n",
    "#     for i in range(0, n_out):\n",
    "#         cols.append(df.shift(-i))\n",
    "#         if i == 0:\n",
    "#             names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "#         else:\n",
    "#             names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "#     # put it all together\n",
    "#     agg = concat(cols, axis=1)\n",
    "#     agg.columns = names\n",
    "#     # drop rows with NaN values\n",
    "#     if dropnan:\n",
    "#         agg.dropna(inplace=True)\n",
    "#     return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "further-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5226481  0.58082193 0.5       ]\n",
      " [0.         0.5261324  0.58082193 0.5       ]\n",
      " [0.         0.5296167  0.58082193 0.5       ]\n",
      " ...\n",
      " [0.05       0.9930314  0.         0.8333334 ]\n",
      " [0.05       0.9965157  0.         0.8333334 ]\n",
      " [0.05       1.         0.         0.8333334 ]]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('datasets/bss/dublin/reorg/station_2.csv')\n",
    "# dataset = read_csv('datasets/bss/dublin/reorg_plus_weather/station_2.csv')\n",
    "\n",
    "dataset = dataset.drop('TIME', axis=1)\n",
    "# dataset = dataset.drop('date', axis=1)\n",
    "# print(dataset.head())\n",
    "values = dataset.values\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = scaled\n",
    "\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "genetic-abuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 1, 3) (8760,) (8927, 1, 3) (8927,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "# values = reframed.values\n",
    "\n",
    "train_start = 0\n",
    "train_end = 8760\n",
    "\n",
    "test_start = 99144\n",
    "test_end = 108071\n",
    "\n",
    "n_train_hours = 365 * 24\n",
    "train = scaled[train_start:train_end, :]\n",
    "test = scaled[test_start:test_end, :]\n",
    "# train = values[train_start:train_end, :]\n",
    "# test = values[test_start:test_end, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, 1:], train[:, 1]\n",
    "test_X, test_y = test[:, 1:], test[:, 1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "responsible-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "122/122 [==============================] - 6s 23ms/step - loss: 0.3540 - val_loss: 0.1751\n",
      "Epoch 2/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1604 - val_loss: 0.1098\n",
      "Epoch 3/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0883 - val_loss: 0.0187\n",
      "Epoch 4/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0136 - val_loss: 0.0086\n",
      "Epoch 5/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0094 - val_loss: 0.0078\n",
      "Epoch 6/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0085 - val_loss: 0.0070\n",
      "Epoch 7/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0076 - val_loss: 0.0062\n",
      "Epoch 8/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0068 - val_loss: 0.0053\n",
      "Epoch 9/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0063 - val_loss: 0.0046\n",
      "Epoch 10/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0054 - val_loss: 0.0089\n",
      "Epoch 11/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0096 - val_loss: 0.0052\n",
      "Epoch 12/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0049\n",
      "Epoch 13/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0040 - val_loss: 0.0059\n",
      "Epoch 14/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 15/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0040 - val_loss: 0.0080\n",
      "Epoch 16/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0093 - val_loss: 0.0061\n",
      "Epoch 17/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0045 - val_loss: 0.0020\n",
      "Epoch 18/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0031 - val_loss: 0.0057\n",
      "Epoch 19/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0021\n",
      "Epoch 20/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 21/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 22/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0051 - val_loss: 0.0037\n",
      "Epoch 23/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0111\n",
      "Epoch 24/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0059\n",
      "Epoch 25/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0039 - val_loss: 0.0044\n",
      "Epoch 26/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0055 - val_loss: 0.0064\n",
      "Epoch 27/200\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.005 - 1s 4ms/step - loss: 0.0056 - val_loss: 0.0033\n",
      "Epoch 28/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 29/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0033 - val_loss: 0.0045\n",
      "Epoch 30/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0039 - val_loss: 0.0064\n",
      "Epoch 31/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 32/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0021\n",
      "Epoch 33/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0028\n",
      "Epoch 34/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0025 - val_loss: 0.0061\n",
      "Epoch 35/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0045 - val_loss: 0.0068\n",
      "Epoch 36/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0080\n",
      "Epoch 37/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0070 - val_loss: 0.0076\n",
      "Epoch 38/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0074 - val_loss: 0.0128\n",
      "Epoch 39/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0074 - val_loss: 0.0045\n",
      "Epoch 40/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0021 - val_loss: 0.0034\n",
      "Epoch 41/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 42/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0030 - val_loss: 0.0023\n",
      "Epoch 43/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 44/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 45/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0028 - val_loss: 0.0046\n",
      "Epoch 46/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0041 - val_loss: 0.0028\n",
      "Epoch 47/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0054 - val_loss: 0.0081\n",
      "Epoch 48/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0062 - val_loss: 0.0090\n",
      "Epoch 49/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0082 - val_loss: 0.0033\n",
      "Epoch 50/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0036 - val_loss: 0.0064\n",
      "Epoch 51/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0049\n",
      "Epoch 52/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0041 - val_loss: 0.0055\n",
      "Epoch 53/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0044 - val_loss: 0.0031\n",
      "Epoch 54/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0051 - val_loss: 0.0080\n",
      "Epoch 55/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0068 - val_loss: 0.0050\n",
      "Epoch 56/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0045 - val_loss: 0.0026\n",
      "Epoch 57/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0050 - val_loss: 0.0044\n",
      "Epoch 58/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 59/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0040\n",
      "Epoch 60/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 61/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0047\n",
      "Epoch 62/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 63/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0029 - val_loss: 0.0024\n",
      "Epoch 64/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0031 - val_loss: 0.0075\n",
      "Epoch 65/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0067 - val_loss: 0.0024\n",
      "Epoch 66/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0023 - val_loss: 0.0090\n",
      "Epoch 67/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0064 - val_loss: 0.0114\n",
      "Epoch 68/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0075 - val_loss: 0.0058\n",
      "Epoch 69/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0064 - val_loss: 0.0043\n",
      "Epoch 70/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0030 - val_loss: 0.0062\n",
      "Epoch 71/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 72/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0056 - val_loss: 0.0031\n",
      "Epoch 73/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0049 - val_loss: 0.0063\n",
      "Epoch 74/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0038 - val_loss: 0.0048\n",
      "Epoch 75/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0033 - val_loss: 0.0026\n",
      "Epoch 76/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0061 - val_loss: 0.0077\n",
      "Epoch 77/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0063 - val_loss: 0.0042\n",
      "Epoch 78/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 79/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0041 - val_loss: 0.0119\n",
      "Epoch 80/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0083 - val_loss: 0.0043\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0039 - val_loss: 0.0083\n",
      "Epoch 82/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0076 - val_loss: 0.0042\n",
      "Epoch 83/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0031 - val_loss: 0.0061\n",
      "Epoch 84/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0024\n",
      "Epoch 85/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0028 - val_loss: 0.0066\n",
      "Epoch 86/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0065 - val_loss: 0.0038\n",
      "Epoch 87/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0043 - val_loss: 0.0065\n",
      "Epoch 88/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 89/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 90/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0032 - val_loss: 0.0097\n",
      "Epoch 91/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0063 - val_loss: 0.0027\n",
      "Epoch 92/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0033 - val_loss: 0.0029\n",
      "Epoch 93/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0055\n",
      "Epoch 94/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0034\n",
      "Epoch 95/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0013\n",
      "Epoch 96/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0056\n",
      "Epoch 97/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0051\n",
      "Epoch 98/200\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.004 - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0033\n",
      "Epoch 99/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0014\n",
      "Epoch 100/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 101/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0046\n",
      "Epoch 102/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0067\n",
      "Epoch 103/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 104/200\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.004 - 0s 4ms/step - loss: 0.0048 - val_loss: 0.0026\n",
      "Epoch 105/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0032 - val_loss: 0.0088\n",
      "Epoch 106/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0044 - val_loss: 0.0026\n",
      "Epoch 107/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0043 - val_loss: 0.0056\n",
      "Epoch 108/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0020\n",
      "Epoch 109/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0040\n",
      "Epoch 110/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0062 - val_loss: 0.0094\n",
      "Epoch 111/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0051 - val_loss: 0.0027\n",
      "Epoch 112/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0028 - val_loss: 0.0060\n",
      "Epoch 113/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0043 - val_loss: 0.0084\n",
      "Epoch 114/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0068 - val_loss: 0.0032\n",
      "Epoch 115/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0033 - val_loss: 0.0088\n",
      "Epoch 116/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.0033\n",
      "Epoch 117/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0033 - val_loss: 0.0066\n",
      "Epoch 118/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0115\n",
      "Epoch 119/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0024\n",
      "Epoch 120/200\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.005 - 1s 4ms/step - loss: 0.0051 - val_loss: 0.0021\n",
      "Epoch 121/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0026 - val_loss: 0.0056\n",
      "Epoch 122/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0050 - val_loss: 0.0025\n",
      "Epoch 123/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0033 - val_loss: 0.0081\n",
      "Epoch 124/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0063 - val_loss: 0.0028\n",
      "Epoch 125/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 126/200\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.003 - 1s 4ms/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 127/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0075\n",
      "Epoch 128/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 129/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 130/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0051 - val_loss: 0.0015\n",
      "Epoch 131/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 132/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 133/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0027 - val_loss: 0.0102\n",
      "Epoch 134/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0084 - val_loss: 0.0028\n",
      "Epoch 135/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0043 - val_loss: 0.0067\n",
      "Epoch 136/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 137/200\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.003 - 1s 4ms/step - loss: 0.0038 - val_loss: 0.0029\n",
      "Epoch 138/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0054 - val_loss: 0.0095\n",
      "Epoch 139/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0053 - val_loss: 0.0027\n",
      "Epoch 140/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 141/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0046\n",
      "Epoch 142/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 143/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0027\n",
      "Epoch 144/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0056\n",
      "Epoch 145/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 146/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0082\n",
      "Epoch 147/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 148/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0070 - val_loss: 0.0041\n",
      "Epoch 149/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0046\n",
      "Epoch 150/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0033 - val_loss: 0.0074\n",
      "Epoch 151/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0025\n",
      "Epoch 152/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0030 - val_loss: 0.0063\n",
      "Epoch 153/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0072\n",
      "Epoch 154/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0052 - val_loss: 0.0055\n",
      "Epoch 155/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0039 - val_loss: 0.0060\n",
      "Epoch 156/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0037\n",
      "Epoch 157/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0041 - val_loss: 0.0063\n",
      "Epoch 158/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0070\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 160/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 161/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0017\n",
      "Epoch 162/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0027 - val_loss: 0.0044\n",
      "Epoch 163/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0049 - val_loss: 0.0024\n",
      "Epoch 164/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0038 - val_loss: 0.0023\n",
      "Epoch 165/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0025 - val_loss: 0.0036\n",
      "Epoch 166/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0028 - val_loss: 0.0033\n",
      "Epoch 167/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 168/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 169/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0033 - val_loss: 0.0049\n",
      "Epoch 170/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0046 - val_loss: 0.0022\n",
      "Epoch 171/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 172/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 173/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0024 - val_loss: 0.0040\n",
      "Epoch 174/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0064\n",
      "Epoch 175/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0047 - val_loss: 0.0019\n",
      "Epoch 176/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 177/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0020 - val_loss: 0.0039\n",
      "Epoch 178/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0034 - val_loss: 0.0060\n",
      "Epoch 179/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0046 - val_loss: 0.0036\n",
      "Epoch 180/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0026 - val_loss: 0.0038\n",
      "Epoch 181/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 182/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0042 - val_loss: 0.0068\n",
      "Epoch 183/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 184/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0018\n",
      "Epoch 185/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 186/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0038\n",
      "Epoch 187/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 188/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 189/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 190/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0032 - val_loss: 0.0024\n",
      "Epoch 191/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0063\n",
      "Epoch 192/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 193/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0058 - val_loss: 0.0088\n",
      "Epoch 194/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0057\n",
      "Epoch 195/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0054 - val_loss: 0.0024\n",
      "Epoch 196/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0031 - val_loss: 0.0025\n",
      "Epoch 197/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 198/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 199/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 200/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0024 - val_loss: 0.0041\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y,\n",
    "                    epochs=150,\n",
    "                    batch_size=72,\n",
    "                    validation_data=(test_X, test_y),\n",
    "                    verbose=1,\n",
    "                    shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "palestinian-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.082\n",
      "Test MSE: 0.009\n",
      "Test RMSE: 0.096\n",
      "Test R2: 0.999728121313571360673222443438\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "# print(test_X)\n",
    "# print(yhat)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 0]\n",
    "# calculate RMSE\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# temp = concatenate((inv_y, inv_yhat))\n",
    "# print(temp)\n",
    "# print(inv_y)\n",
    "# print(inv_yhat)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "mse = mean_squared_error(inv_y, inv_yhat)\n",
    "r2 = r2_score(inv_y, inv_yhat)\n",
    "print('Test MAE: %.3f' % mae)\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test R2: %.30f' % r2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
