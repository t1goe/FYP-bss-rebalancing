{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominican-constitution",
   "metadata": {},
   "source": [
    "# LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mobile-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "# def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "#     n_vars = 1 if type(data) is list else data.shape[1]\n",
    "#     df = DataFrame(data)\n",
    "#     cols, names = list(), list()\n",
    "#     # input sequence (t-n, ... t-1)\n",
    "#     for i in range(n_in, 0, -1):\n",
    "#         cols.append(df.shift(i))\n",
    "#         names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "#     # forecast sequence (t, t+1, ... t+n)\n",
    "#     for i in range(0, n_out):\n",
    "#         cols.append(df.shift(-i))\n",
    "#         if i == 0:\n",
    "#             names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "#         else:\n",
    "#             names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "#     # put it all together\n",
    "#     agg = concat(cols, axis=1)\n",
    "#     agg.columns = names\n",
    "#     # drop rows with NaN values\n",
    "#     if dropnan:\n",
    "#         agg.dropna(inplace=True)\n",
    "#     return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "further-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5226481  0.58082193 0.5       ]\n",
      " [0.         0.5261324  0.58082193 0.5       ]\n",
      " [0.         0.5296167  0.58082193 0.5       ]\n",
      " ...\n",
      " [0.05       0.9930314  0.         0.8333334 ]\n",
      " [0.05       0.9965157  0.         0.8333334 ]\n",
      " [0.05       1.         0.         0.8333334 ]]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('../datasets/bss/dublin/reorg/station_2.csv')\n",
    "# dataset = read_csv('datasets/bss/dublin/reorg_plus_weather/station_2.csv')\n",
    "\n",
    "dataset = dataset.drop('TIME', axis=1)\n",
    "# dataset = dataset.drop('date', axis=1)\n",
    "# print(dataset.head())\n",
    "values = dataset.values\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = scaled\n",
    "\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "genetic-abuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 1, 3) (8760,) (8927, 1, 3) (8927,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "# values = reframed.values\n",
    "\n",
    "train_start = 0\n",
    "train_end = 8760\n",
    "\n",
    "test_start = 99144\n",
    "test_end = 108071\n",
    "\n",
    "n_train_hours = 365 * 24\n",
    "train = scaled[train_start:train_end, :]\n",
    "test = scaled[test_start:test_end, :]\n",
    "# train = values[train_start:train_end, :]\n",
    "# test = values[test_start:test_end, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, 1:], train[:, 1]\n",
    "test_X, test_y = test[:, 1:], test[:, 1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "122/122 [==============================] - 7s 24ms/step - loss: 0.3488 - val_loss: 0.1752\n",
      "Epoch 2/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1607 - val_loss: 0.1102\n",
      "Epoch 3/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0889 - val_loss: 0.0181\n",
      "Epoch 4/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0142 - val_loss: 0.0092\n",
      "Epoch 5/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0099 - val_loss: 0.0084\n",
      "Epoch 6/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0091 - val_loss: 0.0077\n",
      "Epoch 7/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0083 - val_loss: 0.0069\n",
      "Epoch 8/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0074 - val_loss: 0.0060\n",
      "Epoch 9/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0067 - val_loss: 0.0054\n",
      "Epoch 10/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 11/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0091 - val_loss: 0.0048\n",
      "Epoch 12/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0054 - val_loss: 0.0101\n",
      "Epoch 13/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0096 - val_loss: 0.0067\n",
      "Epoch 14/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 15/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0057 - val_loss: 0.0118\n",
      "Epoch 16/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0088 - val_loss: 0.0057\n",
      "Epoch 17/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0067 - val_loss: 0.0090\n",
      "Epoch 18/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0070 - val_loss: 0.0043\n",
      "Epoch 19/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0061 - val_loss: 0.0045\n",
      "Epoch 20/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 21/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0052 - val_loss: 0.0031\n",
      "Epoch 22/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0037 - val_loss: 0.0070\n",
      "Epoch 23/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0045 - val_loss: 0.0077\n",
      "Epoch 24/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0063 - val_loss: 0.0051\n",
      "Epoch 25/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0043 - val_loss: 0.0027\n",
      "Epoch 26/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0053 - val_loss: 0.0060\n",
      "Epoch 27/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 28/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 29/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0034 - val_loss: 0.0038\n",
      "Epoch 30/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 31/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0064 - val_loss: 0.0095\n",
      "Epoch 32/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0070 - val_loss: 0.0054\n",
      "Epoch 33/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0071\n",
      "Epoch 34/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0050 - val_loss: 0.0066\n",
      "Epoch 35/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 36/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 37/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 38/150\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 39/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0045 - val_loss: 0.0030\n",
      "Epoch 40/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0044 - val_loss: 0.0016\n",
      "Epoch 41/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0040 - val_loss: 0.0026\n",
      "Epoch 42/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0050\n",
      "Epoch 43/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0068 - val_loss: 0.0041\n",
      "Epoch 44/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0032 - val_loss: 0.0048\n",
      "Epoch 45/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0072 - val_loss: 0.0084\n",
      "Epoch 46/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0083 - val_loss: 0.0068\n",
      "Epoch 47/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0052 - val_loss: 0.0035\n",
      "Epoch 48/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 49/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0043 - val_loss: 0.0053\n",
      "Epoch 50/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0061 - val_loss: 0.0040\n",
      "Epoch 51/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0062 - val_loss: 0.0023\n",
      "Epoch 52/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0062\n",
      "Epoch 53/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0057 - val_loss: 0.0104\n",
      "Epoch 54/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0053 - val_loss: 0.0020\n",
      "Epoch 55/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0049\n",
      "Epoch 56/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0050 - val_loss: 0.0031\n",
      "Epoch 57/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0045 - val_loss: 0.0026\n",
      "Epoch 58/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0034 - val_loss: 0.0076\n",
      "Epoch 59/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0058 - val_loss: 0.0031\n",
      "Epoch 60/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0037 - val_loss: 0.0064\n",
      "Epoch 61/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0040\n",
      "Epoch 62/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0039 - val_loss: 0.0050\n",
      "Epoch 63/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 64/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0062\n",
      "Epoch 65/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0048 - val_loss: 0.0079\n",
      "Epoch 66/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 67/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0050 - val_loss: 0.0037\n",
      "Epoch 68/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0053 - val_loss: 0.0076\n",
      "Epoch 69/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0083 - val_loss: 0.0049\n",
      "Epoch 70/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0027 - val_loss: 0.0069\n",
      "Epoch 71/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0050 - val_loss: 0.0028\n",
      "Epoch 72/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0097\n",
      "Epoch 73/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0055 - val_loss: 0.0067\n",
      "Epoch 74/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0051 - val_loss: 0.0063\n",
      "Epoch 75/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0041 - val_loss: 0.0014\n",
      "Epoch 76/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0040 - val_loss: 0.0053\n",
      "Epoch 77/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 78/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0053 - val_loss: 0.0095\n",
      "Epoch 79/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0055 - val_loss: 0.0029\n",
      "Epoch 80/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0026 - val_loss: 0.0096\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0064 - val_loss: 0.0022\n",
      "Epoch 82/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0051 - val_loss: 0.0019\n",
      "Epoch 83/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 84/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 85/150\n",
      " 38/122 [========>.....................] - ETA: 0s - loss: 0.0038"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y,\n",
    "                    epochs=150,\n",
    "                    batch_size=72,\n",
    "                    validation_data=(test_X, test_y),\n",
    "                    verbose=1,\n",
    "                    shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "# print(test_X)\n",
    "# print(yhat)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 0]\n",
    "# calculate RMSE\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# temp = concatenate((inv_y, inv_yhat))\n",
    "# print(temp)\n",
    "# print(inv_y)\n",
    "# print(inv_yhat)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "mse = mean_squared_error(inv_y, inv_yhat)\n",
    "r2 = r2_score(inv_y, inv_yhat)\n",
    "print('Test MAE: %.3f' % mae)\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test R2: %.30f' % r2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
