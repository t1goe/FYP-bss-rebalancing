{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-volunteer",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-pacific",
   "metadata": {},
   "source": [
    "While the dataset is easily accessable from smartdublin and met eireann's websites, the data must be correctly preprocessed before use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-tuner",
   "metadata": {},
   "source": [
    "Imports for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "happy-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "import wget\n",
    "from pandas.errors import DtypeWarning\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from numpy import nan\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-transformation",
   "metadata": {},
   "source": [
    "Downloads the data from their respective websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "guilty-linux",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dublin weather data already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dublin_weather():\n",
    "    \"\"\"\n",
    "    Download dublin weather data from met eireann\n",
    "    \"\"\"\n",
    "\n",
    "    destination = '../datasets/weather'\n",
    "    url = \"https://cli.fusio.net/cli/climate_data/webdata/hly175.zip\"\n",
    "\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    # Download if file does not exist\n",
    "    # Downloads as zip, so code exists to unzip and remove it\n",
    "    if not os.path.isfile(destination + \"/hly175/hly175.csv\"):\n",
    "        print(\"Downloading hourly Dublin weather data\")\n",
    "        wget.download(url, destination)\n",
    "        with zipfile.ZipFile(destination + \"/hly175.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(destination + \"/hly175\")\n",
    "        os.remove(destination + \"/hly175.zip\")\n",
    "    else:\n",
    "        print(\"Dublin weather data already exists\\n\")\n",
    "        \n",
    "dublin_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "structural-paintball",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 3659.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dublinbikes data\n",
      "Finished downloading Dublin BSS data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def dublin_bss():\n",
    "    \"\"\"\n",
    "    Download dublin bss data with wget\n",
    "    \"\"\"\n",
    "    destination = '../datasets/bss/dublin/original/'\n",
    "\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    base_url = \"https://data.smartdublin.ie/dataset/33ec9fe2-4957-4e9a-ab55-c5e917c7a9ab/resource\"\n",
    "    urls = [\"/99a35442-6878-4c2d-8dff-ec43e91d21d7/download/dublinbikes_20200701_20201001.csv\",\n",
    "            \"/8ddaeac6-4caf-4289-9835-cf588d0b69e5/download/dublinbikes_20200401_20200701.csv\",\n",
    "            \"/aab12e7d-547f-463a-86b1-e22002884587/download/dublinbikes_20200101_20200401.csv\",\n",
    "            \"/5d23332e-4f49-4c41-b6a0-bffb77b33d64/download/dublinbikes_20191001_20200101.csv\",\n",
    "            \"/305d39ac-b6a0-4216-a535-0ae2ddf59819/download/dublinbikes_20190701_20191001.csv\",\n",
    "            \"/76fdda3d-d8be-441b-92dd-0ee36d9c5316/download/dublinbikes_20190401_20190701.csv\",\n",
    "            \"/538165d7-535e-4e1d-909a-1c1bfae901c5/download/dublinbikes_20190101_20190401.csv\",\n",
    "            \"/67ea095f-67ad-47f5-b8f7-044743043848/download/dublinbikes_20181001_20190101.csv\",\n",
    "            \"/9496fac5-e4d7-4ae9-a49a-217c7c4e83d9/download/dublinbikes_20180701_20181001.csv\",\n",
    "            \"/5328239f-bcc6-483d-9c17-87166efc3a1a/download/dublinbikes_20201001_20210101.csv\",\n",
    "            \"/2dec86ed-76ed-47a3-ae28-646db5c5b965/download/dublin.csv\"]\n",
    "\n",
    "    print(\"Downloading dublinbikes data\")\n",
    "    for url in tqdm(urls):\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        if filename in os.listdir(destination):\n",
    "            # print(\"File \\\"\" + filename + \"\\\" already exists\")\n",
    "            continue\n",
    "\n",
    "        final_url = base_url + url\n",
    "        # print(\"starting download on \" + filename)\n",
    "        wget.download(final_url, destination)\n",
    "        # print(filename + \" : downloaded\\n\")\n",
    "\n",
    "    print(\"Finished downloading Dublin BSS data\")\n",
    "\n",
    "    # Clear generated tmp files\n",
    "    for file in os.listdir():\n",
    "        if file[-4:] == \".tmp\":\n",
    "            os.remove(file)\n",
    "            \n",
    "dublin_bss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unavailable-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_ids():\n",
    "    # Get all the station IDs\n",
    "    dataset = pd.read_csv('../datasets/bss/dublin/original/dublin.csv',\n",
    "                          usecols=['Number'])\n",
    "    station_ids = []\n",
    "    for d in dataset['Number'].unique():\n",
    "        station_ids.append(str(d))\n",
    "    station_ids.sort()\n",
    "    return station_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-chile",
   "metadata": {},
   "source": [
    "This function reorganizes all the data into CSVs sorted by station instead of yearly quarter. This is to make the data easier to work with, as the size of the dataset means that it is easiet to process the data on a station-by-station basis. This part also removes columns that are either redundant (latitude/longitude) or do not add anything to the analysis (\"LAST UPDATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "apart-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 3438.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dublinbikes reorganisation\n",
      "\n",
      "Finished dublinbikes reorganisation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_time(x):\n",
    "    \"\"\"\n",
    "    Converts TIME field in the CSV to an integer representing\n",
    "    what time of day it is (in number of 5min increments) from 0 to 287\n",
    "    eg\n",
    "    - 00:00 -> 0\n",
    "    - 00:10 -> 2\n",
    "    - 02:20 -> 28\n",
    "    etc\n",
    "    \"\"\"\n",
    "    a = x.split(' ')\n",
    "    a = a[1].split(':')\n",
    "\n",
    "    ans = math.floor((int(a[0]) * 12) + (int(a[1]) / 5))\n",
    "\n",
    "    return ans\n",
    "\n",
    "def convert_date(x):\n",
    "    \"\"\"\n",
    "    Converts TIME field to an integer representing the day of the year\n",
    "    \n",
    "    eg\n",
    "    - 2019-02-10 -> 41\n",
    "    \"\"\"\n",
    "    current_date = datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return current_date.strftime('%j')\n",
    "\n",
    "def convert_day(x):\n",
    "    \"\"\"\n",
    "    Converts TIME field to an integer representing the day of the year\n",
    "    \n",
    "    eg\n",
    "    - 2019-02-10 -> 41\n",
    "    \"\"\"\n",
    "    current_date = datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return current_date.strftime('%w')\n",
    "    \n",
    "\n",
    "def organise_by_station():\n",
    "    \"\"\"\n",
    "    Reorganise all the dublinbikes CSVs by station instead of by quarter\n",
    "    \"\"\"\n",
    "    # Get list of data files\n",
    "    bss_files = os.listdir('../datasets/bss/dublin/original')\n",
    "    if 'dublin.csv' in bss_files:\n",
    "        bss_files.remove('dublin.csv')\n",
    "\n",
    "    if 'reorg' in bss_files:\n",
    "        bss_files.remove('reorg')\n",
    "\n",
    "    # Get all the station IDs\n",
    "    dataset = pd.read_csv('../datasets/bss/dublin/original/dublin.csv',\n",
    "                          usecols=['Number'])\n",
    "    station_ids = []\n",
    "    for d in dataset['Number'].unique():\n",
    "        station_ids.append(d)\n",
    "    station_ids.sort()\n",
    "\n",
    "    # Get column names\n",
    "    columns = pd.read_csv('../datasets/bss/dublin/original/dublinbikes_20200701_20201001.csv', nrows=1).columns\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    destination = '../datasets/bss/dublin/reorg/'\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    print(\"Starting dublinbikes reorganisation\")\n",
    "    \n",
    "    for station in tqdm(station_ids):\n",
    "        if os.path.exists('../datasets/bss/dublin/reorg/station_' + str(station) + '.csv'):\n",
    "            # print('\\tStation CSV already exists')\n",
    "            continue\n",
    "\n",
    "#         print('Working on station: ' + str(station))\n",
    "        df1 = pd.DataFrame(columns=columns)\n",
    "        for file in bss_files:\n",
    "            df2 = pd.read_csv('../datasets/bss/dublin/original/' + str(file))\n",
    "            df2 = df2[df2['STATION ID'] == station]\n",
    "            temp = [df1, df2]\n",
    "            df1 = pd.concat(temp)\n",
    "        df1 = df1.drop(df1.columns[[0]], axis=1)\n",
    "        df1 = df1.drop([\n",
    "            'LAST UPDATED',\n",
    "            'NAME',\n",
    "            'BIKE STANDS',\n",
    "            'AVAILABLE BIKE STANDS',\n",
    "            'STATUS',\n",
    "            'ADDRESS',\n",
    "            'LATITUDE',\n",
    "            'LONGITUDE'\n",
    "        ], axis=1)\n",
    "        \n",
    "        df1['int_time'] = df1['TIME'].apply(lambda x: convert_time(x))\n",
    "        df1['int_date'] = df1['TIME'].apply(lambda x: convert_date(x))\n",
    "        df1['int_day'] = df1['TIME'].apply(lambda x: convert_day(x))\n",
    "        \n",
    "        # Remove duplicate april 1st\n",
    "        df1 = df1.drop_duplicates()\n",
    "        \n",
    "        df1.to_csv('../datasets/bss/dublin/reorg/station_' + str(station) + '.csv', index=False)\n",
    "\n",
    "    print('\\nFinished dublinbikes reorganisation')\n",
    "    \n",
    "organise_by_station()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-light",
   "metadata": {},
   "source": [
    "The first 15 rows in the downloaded CSV contains information describing the dataset, this is removed before processing to allow us to use the dataset with pandas. Much like the station data I also remove some columns that have no use for the project (like msl which is\"Mean Sea Level Pressure\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sitting-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather_data():\n",
    "    # Remove first n rows, that contain data from before the start of the BSS data\n",
    "    with open('../datasets/weather/hly175/hly175.csv', 'r') as fin:\n",
    "        data = fin.read().splitlines(True)\n",
    "    with open('../datasets/weather/hly175/hly175clean.csv', 'w') as fout:\n",
    "        fout.writelines(data[15])\n",
    "        fout.writelines(data[131164:])\n",
    "\n",
    "    # Remove all rows with non-useful data\n",
    "    warnings.filterwarnings(\"ignore\", category=DtypeWarning)\n",
    "    dataset = pd.read_csv('../datasets/weather/hly175/hly175clean.csv',\n",
    "                          usecols=['date', 'rain', 'temp', 'rhum'])\n",
    "\n",
    "    #Was used for an old approach for joining weather and station data\n",
    "    #dataset[\"epoch\"] = dataset[\"date\"].apply(lambda x: int(time.mktime(time.strptime(x, \"%d-%b-%Y %H:%M\"))))\n",
    "\n",
    "    dataset.to_csv('../datasets/weather/hly175/hly175clean.csv', index=False)\n",
    "    \n",
    "clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-lodging",
   "metadata": {},
   "source": [
    "Many rows in the station data is missing, this function replcaes them with null rows. This is necessary to ensure that the weather data lines up correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "respected-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 2245.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def add_null_rows(station_number):\n",
    "    \n",
    "    station_file = \"../datasets/bss/dublin/reorg/station_\" + station_number + \".csv\"\n",
    "    \n",
    "    if not os.path.exists(station_file):\n",
    "        # print(\"File: \" + str(station_file) + \" does not exist\")\n",
    "        return\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    destination = '../datasets/bss/dublin/reorg_w_null/'\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    if os.path.exists(destination + station_file.split(\"/\")[-1]):\n",
    "        # print(\"File plus weather already exists\")\n",
    "        return\n",
    "\n",
    "    station = pd.read_csv(station_file)\n",
    "\n",
    "    start_time = datetime.strptime(station['TIME'].iloc[0][:-3], \"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    pos = 0\n",
    "    with open(destination + '/' + station_file.split('/')[-1], 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = station.columns\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        empty_row = {}\n",
    "        current_row = {}\n",
    "        for c in station.columns:\n",
    "            empty_row[c] = nan\n",
    "            current_row[c] = nan\n",
    "\n",
    "\n",
    "        #         for index, row in tqdm(station.iterrows(), total=station.shape[0]):\n",
    "        for index, row in station.iterrows():\n",
    "\n",
    "            target_time = start_time + timedelta(minutes=pos * 5)\n",
    "            str_time = row['TIME'][:-3]\n",
    "\n",
    "            if int(str_time[-1]) >= 5:\n",
    "                str_time = str_time[:-1] + '5'\n",
    "            else:\n",
    "                str_time = str_time[:-1] + '0'\n",
    "\n",
    "            current_time = datetime.strptime(str_time, \"%Y-%m-%d %H:%M\")\n",
    "            while current_time > target_time:\n",
    "                writer.writerow(empty_row)\n",
    "\n",
    "                pos = pos + 1\n",
    "                target_time = start_time + timedelta(minutes=pos * 5)\n",
    "\n",
    "            for c in station.columns:\n",
    "                current_row[c] = row[c]\n",
    "\n",
    "            # writer.writerow({\n",
    "            #     'TIME': row['TIME'],\n",
    "            #     'BIKE STANDS': row['BIKE STANDS'],\n",
    "            #     'AVAILABLE BIKE STANDS': row['AVAILABLE BIKE STANDS'],\n",
    "            #     'AVAILABLE BIKES': row['AVAILABLE BIKES'],\n",
    "            # })\n",
    "            writer.writerow(current_row)\n",
    "            pos = pos + 1\n",
    "            \n",
    "station_ids = get_station_ids()\n",
    "for station in tqdm(station_ids):\n",
    "    add_null_rows(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minute-conservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [00:43<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def attach_weather_data(station_number):\n",
    "    \n",
    "    source_file = \"../datasets/bss/dublin/reorg_w_null/station_\" + station_number + \".csv\"\n",
    "    \n",
    "    if not os.path.exists(source_file):\n",
    "#         print(\"File \" + str(source_file) + \" does not exist in /reorg_w_null\")\n",
    "        return\n",
    "\n",
    "    station_dataframe = pd.read_csv(source_file)\n",
    "\n",
    "    destination_file = '../datasets/bss/dublin/reorg_plus_weather/station_' + station_number + '.csv'\n",
    "\n",
    "    destination_directory = '../datasets/bss/dublin/reorg_plus_weather/'\n",
    "\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    if os.path.exists(destination_file):\n",
    "        # print(\"File plus weather already exists\")\n",
    "        return\n",
    "\n",
    "    weather = pd.read_csv('../datasets/weather/hly175/hly175clean.csv')\n",
    "\n",
    "    new_weather_dataframe = pd.DataFrame(np.repeat(weather.values, 12, axis=0), columns=weather.columns)\n",
    "    \n",
    "    result = pd.concat([station_dataframe, new_weather_dataframe], axis=1)\n",
    "    \n",
    "    result = result.drop(['date'], axis=1)\n",
    "    \n",
    "    result = result.dropna()\n",
    "\n",
    "    result.to_csv('../datasets/bss/dublin/reorg_plus_weather/station_' + station_number + '.csv', index=False)\n",
    "\n",
    "# mypath = \"./datasets/bss/dublin/reorg_w_null/\"\n",
    "\n",
    "# files = [f for f in os.listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "# for file in tqdm(files):\n",
    "#     attach_weather_data(mypath, file)\n",
    "    \n",
    "station_ids = get_station_ids()\n",
    "for station in tqdm(station_ids):\n",
    "    attach_weather_data(station)\n",
    "\n",
    "# attach_weather_data(mypath, \"station_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-payday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
