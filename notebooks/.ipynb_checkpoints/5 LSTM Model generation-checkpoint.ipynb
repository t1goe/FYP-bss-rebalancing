{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominican-constitution",
   "metadata": {},
   "source": [
    "# LSTM Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fluid-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_date(df, date):\n",
    "    # print(date)\n",
    "    x = df.index[df['DATE'] == str(date).split(' ')[0]].tolist()\n",
    "    if len(x) == 0:\n",
    "        print(\"Date: \" + str(date) + \" not found in dataset\")\n",
    "        if date.year <= 2018:\n",
    "            print(\"Assuming before start of dataset, returning 0\")\n",
    "            return 0\n",
    "        elif date.year >= 2020:\n",
    "            print(\"Assuming after end of dataset, returning end\")\n",
    "            return (len(df) - 1)\n",
    "            \n",
    "\n",
    "    return x[0]\n",
    "\n",
    "def get_data_split(\n",
    "                file_location,\n",
    "                train_start_date=datetime(year=2018, month=8, day=1),\n",
    "                train_end_date=datetime(year=2019, month=7, day=30),\n",
    "                test_start_date=datetime(year=2019, month=8, day=1),\n",
    "                test_end_date=datetime(year=2019, month=12, day=31),\n",
    "                cols_to_use=None\n",
    "                ):\n",
    "    if cols_to_use is None:\n",
    "        cols_to_use = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "        ]\n",
    "\n",
    "    cols_to_use.insert(0, 'AVAILABLE BIKES')\n",
    "    cols_to_use.insert(0, 'TIME')\n",
    "    # load dataset\n",
    "    dataset = read_csv(file_location, usecols=cols_to_use)\n",
    "    dataset['DATE'] = dataset['TIME'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "    if 'rain' in cols_to_use:\n",
    "        dataset = dataset[dataset['rain'].str.strip().astype(bool)]\n",
    "\n",
    "    train_start_index = (get_index_of_date(dataset, train_start_date))\n",
    "    train_end_index = (get_index_of_date(dataset, train_end_date))\n",
    "    # print( train_end_index - train_start_index)\n",
    "\n",
    "    test_start_index = (get_index_of_date(dataset, test_start_date))\n",
    "    test_end_index = (get_index_of_date(dataset, test_end_date))\n",
    "    # print(test_end_index - test_start_index)\n",
    "\n",
    "    dataset = dataset.drop(['TIME', 'DATE'], axis=1)\n",
    "    # print(dataset.head())\n",
    "    # print(dataset)\n",
    "    values = dataset.values\n",
    "    # print(values.shape)\n",
    "\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # print(values.shape)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    # frame as supervised learning\n",
    "    reframed = scaled\n",
    "\n",
    "    # print(scaled)\n",
    "\n",
    "    # split into train and test sets\n",
    "    # values = reframed.values\n",
    "\n",
    "    train = scaled[train_start_index:train_end_index, :]\n",
    "    test = scaled[test_start_index:test_end_index, :]\n",
    "    # train = values[train_start:train_end, :]\n",
    "    # test = values[test_start:test_end, :]\n",
    "\n",
    "    # split into input and outputs\n",
    "    train_x, train_y = train[:, 1:], train[:, 0]\n",
    "    test_x, test_y = test[:, 1:], test[:, 0]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "    test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
    "    # print(train_X.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, scaler\n",
    "\n",
    "def get_trained_model(train_x, train_y, test_x, test_y, verbose=1):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_x, train_y,\n",
    "                        epochs=150,\n",
    "                        batch_size=72,\n",
    "                        validation_data=(test_x, test_y),\n",
    "                        verbose=verbose,\n",
    "                        shuffle=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "absent-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▎                                                                         | 10/110 [00:12<02:09,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████                                                                      | 15/110 [00:18<01:57,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|███████████▊                                                                     | 16/110 [00:20<01:55,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▌                                                                    | 17/110 [00:21<01:54,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▉                                                                   | 19/110 [00:22<01:26,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n",
      "Date: 2019-07-30 00:00:00 not found in dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▋                                                                  | 20/110 [00:23<01:10,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n",
      "Date: 2019-07-30 00:00:00 not found in dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [02:14<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "destination_directory = '../datasets/bss/dublin/ml_models/'\n",
    "scaler_destination_directory = '../datasets/bss/dublin/ml_models/scalers/'\n",
    "if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "        \n",
    "if not os.path.exists(scaler_destination_directory):\n",
    "        os.makedirs(scaler_destination_directory)\n",
    "\n",
    "source_directory = '../datasets/bss/dublin/reorg_plus_weather/'\n",
    "files = [f for f in listdir(source_directory) if isfile(join(source_directory, f))]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for file in tqdm(files):\n",
    "# for file in files:\n",
    "    station = file.split('.')[0]\n",
    "    \n",
    "    try:\n",
    "        train_x, train_y, test_x, test_y, scaler = get_data_split(source_directory + file)\n",
    "    except IndexError as e:\n",
    "#         print(\"File \" + file + \" is causing IndexError issues lol\")\n",
    "        continue\n",
    "    except AttributeError as e:\n",
    "#         print(\"File \" + file + \" is causing AttributeError issues lol\")\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(scaler_destination_directory + station + '.pkl'):\n",
    "        file = open(scaler_destination_directory + station + '.pkl', \"w\")\n",
    "        pickle.dump(scaler, file)\n",
    "        file.close()\n",
    "        \n",
    "    if not os.path.exists(destination_directory + station +'.h5'):\n",
    "#         print(\"\\n Working on \" + station)\n",
    "        \n",
    "        model = get_trained_model(train_x, train_y, test_x, test_y, verbose=2)       \n",
    "        model.save(destination_directory + station +'.h5')\n",
    "#     else:\n",
    "#         print(station + \" station model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alpine-slave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/bss/dublin/ml_models/scalers/station_99.pkl\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-81fd5b5225e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaler_destination_directory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'station_2.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaler_destination_directory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstation\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# model = create_model()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = tf.keras.models.load_model(destination_directory + 'station_2.h5')\n",
    "# train_x, train_y, test_x, test_y, scaler = get_data_split('../datasets/bss/dublin/reorg_plus_weather/station_2.csv')\n",
    "file = open(scaler_destination_directory + 'station_2.pkl', \"rb\")\n",
    "print(scaler_destination_directory + station + '.pkl')\n",
    "scaler = pickle.load(file)\n",
    "file.close()\n",
    "# model = create_model()\n",
    "# model.load_weights(destination_directory + 'station_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "\n",
    "yhat = model.predict(test_x)\n",
    "test_x_reshaped = test_x.reshape((test_x.shape[0], test_x.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_x_reshaped), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_x_reshaped), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 0]\n",
    "# calculate RMSE\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# temp = concatenate((inv_y, inv_yhat))\n",
    "# print(temp)\n",
    "# print(inv_y)\n",
    "# print(inv_yhat)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "mse = mean_squared_error(inv_y, inv_yhat)\n",
    "r2 = r2_score(inv_y, inv_yhat)\n",
    "print('Test MAE: %.3f' % mae)\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test R2: %.30f' % r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-harris",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
