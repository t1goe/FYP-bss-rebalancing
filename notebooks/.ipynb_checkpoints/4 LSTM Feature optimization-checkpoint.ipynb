{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "northern-effort",
   "metadata": {},
   "source": [
    "# LSTM Feature optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-hotel",
   "metadata": {},
   "source": [
    "The purposes of this is to explore optimization for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interested-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_date(df, date):\n",
    "    # print(date)\n",
    "    x = df.index[df['DATE'] == str(date).split(' ')[0]].tolist()\n",
    "    if len(x) == 0:\n",
    "        print(\"Date: \" + str(date) + \" not found in dataset\")\n",
    "        exit(1)\n",
    "\n",
    "    return x[0]\n",
    "\n",
    "\n",
    "def output_stats_to_csv(file_location, cols_used, mae, mse, rmse, r2):\n",
    "    col_names = ['int_time',\n",
    "                 'int_date',\n",
    "                 'int_day',\n",
    "                 'rain',\n",
    "                 'temp',\n",
    "                 'rhum',\n",
    "                 'mae',\n",
    "                 'mse',\n",
    "                 'rmse',\n",
    "                 'r2',\n",
    "                 'rmsle',\n",
    "                 'timestamp'\n",
    "                ]\n",
    "    \n",
    "    destination_directory = '../datasets/bss/dublin/feature_optimization_stats/'\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    destination_file = destination_directory + file_location.split('/')[-1]\n",
    "\n",
    "    if not os.path.exists(destination_file):\n",
    "\n",
    "        with open(destination_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows([col_names])\n",
    "\n",
    "    with open(destination_file, 'a', newline='') as csvfile:\n",
    "\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=col_names)\n",
    "\n",
    "        writer.writerow({\n",
    "            'int_time': ('int_time' in cols_used),\n",
    "            'int_date': ('int_date' in cols_used),\n",
    "            'int_day': ('int_day' in cols_used),\n",
    "            'rain': ('rain' in cols_used),\n",
    "            'temp': ('temp' in cols_used),\n",
    "            'rhum': ('rhum' in cols_used),\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'timestamp': datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-president",
   "metadata": {},
   "source": [
    "This is a modified version of the model used to more easily use different combonations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mobile-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(file_location,\n",
    "                train_start_date=datetime(year=2018, month=8, day=1),\n",
    "                train_end_date=datetime(year=2019, month=7, day=30),\n",
    "                test_start_date=datetime(year=2019, month=8, day=1),\n",
    "                test_end_date=datetime(year=2019, month=12, day=31),\n",
    "                cols_to_use=None,\n",
    "                verbose=1\n",
    "                ):\n",
    "    if cols_to_use is None:\n",
    "        cols_to_use = ['int_time', 'int_date', 'int_day']\n",
    "\n",
    "    cols_to_use.insert(0, 'AVAILABLE BIKES')\n",
    "    cols_to_use.insert(0, 'TIME')\n",
    "    \n",
    "    # load dataset\n",
    "    dataset = read_csv(file_location, usecols=cols_to_use)\n",
    "    dataset['DATE'] = dataset['TIME'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "    # Rain causes datatype problems if not dealt with\n",
    "    if 'rain' in cols_to_use:\n",
    "        dataset = dataset[dataset['rain'].str.strip().astype(bool)]\n",
    "\n",
    "    train_start_index = (get_index_of_date(dataset, train_start_date))\n",
    "    train_end_index = (get_index_of_date(dataset, train_end_date))\n",
    "\n",
    "    test_start_index = (get_index_of_date(dataset, test_start_date))\n",
    "    test_end_index = (get_index_of_date(dataset, test_end_date))\n",
    "\n",
    "    dataset = dataset.drop(['TIME', 'DATE'], axis=1)\n",
    "    values = dataset.values\n",
    "\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    \n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "\n",
    "    # split into train and test sets\n",
    "    train = scaled[train_start_index:train_end_index, :]\n",
    "    test = scaled[test_start_index:test_end_index, :]\n",
    "    # train = values[train_start:train_end, :]\n",
    "    # test = values[test_start:test_end, :]\n",
    "\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, 1:], train[:, 0]\n",
    "    test_X, test_y = test[:, 1:], test[:, 0]\n",
    "    \n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(40, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error')\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y,\n",
    "                        epochs=150,\n",
    "                        batch_size=64,\n",
    "                        validation_data=(test_X, test_y),\n",
    "                        verbose=verbose,\n",
    "                        shuffle=False)\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    \n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = concatenate((yhat, test_X), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:, 0]\n",
    "    \n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:, 0]\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "    mse = mean_squared_error(inv_y, inv_yhat)\n",
    "    r2 = r2_score(inv_y, inv_yhat)\n",
    "    \n",
    "    print('Test MAE: %.3f' % mae)\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print('Test R2: %.3f' % r2)\n",
    "\n",
    "    output_stats_to_csv(file_location, cols_to_use, mae, mse, rmse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unavailable-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "\n",
    "\n",
    "def test_powersets(start_position=0,\n",
    "                   file='../datasets/bss/dublin/reorg_plus_weather/station_2.csv',\n",
    "                   train_start_date=None,\n",
    "                   train_end_date=None,\n",
    "                   test_start_date=None,\n",
    "                   test_end_date=None,\n",
    "                   ):\n",
    "    attr_list = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "    ]\n",
    "\n",
    "    y = list(powerset(attr_list))\n",
    "    # y.sort()\n",
    "    print(len(y))\n",
    "    y = sorted(y, key=len)\n",
    "    y.pop(0)\n",
    "\n",
    "    for x in y[start_position:]:\n",
    "        print(str(start_position) + \"/\" + str(len(y) - 1))\n",
    "        start_position = start_position + 1\n",
    "        print(x)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        if train_start_date is not None and train_end_date is not None and test_start_date is not None and test_end_date is not None:\n",
    "            train_model(file,\n",
    "                        train_start_date=train_start_date,\n",
    "                        train_end_date=train_end_date,\n",
    "                        test_start_date=test_start_date,\n",
    "                        test_end_date=test_end_date,\n",
    "                        cols_to_use=x,\n",
    "                        verbose=0)\n",
    "        else:\n",
    "            train_model(file,\n",
    "                        cols_to_use=x,\n",
    "                        verbose=0)\n",
    "        print()\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collaborative-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "0/62\n",
      "['int_time']\n",
      "Test MAE: 2.637\n",
      "Test MSE: 12.603\n",
      "Test RMSE: 3.550\n",
      "Test R2: -0.023\n",
      "\n",
      "1/62\n",
      "['int_date']\n",
      "Test MAE: 2.538\n",
      "Test MSE: 12.336\n",
      "Test RMSE: 3.512\n",
      "Test R2: -0.001\n",
      "\n",
      "2/62\n",
      "['int_day']\n",
      "Test MAE: 2.643\n",
      "Test MSE: 12.458\n",
      "Test RMSE: 3.530\n",
      "Test R2: -0.011\n",
      "\n",
      "3/62\n",
      "['rain']\n",
      "Test MAE: 2.590\n",
      "Test MSE: 12.385\n",
      "Test RMSE: 3.519\n",
      "Test R2: -0.005\n",
      "\n",
      "4/62\n",
      "['temp']\n",
      "Test MAE: 2.724\n",
      "Test MSE: 12.638\n",
      "Test RMSE: 3.555\n",
      "Test R2: -0.026\n",
      "\n",
      "5/62\n",
      "['rhum']\n",
      "Test MAE: 2.667\n",
      "Test MSE: 12.448\n",
      "Test RMSE: 3.528\n",
      "Test R2: -0.010\n",
      "\n",
      "6/62\n",
      "['int_time', 'int_date']\n",
      "Test MAE: 2.629\n",
      "Test MSE: 12.534\n",
      "Test RMSE: 3.540\n",
      "Test R2: -0.017\n",
      "\n",
      "7/62\n",
      "['int_time', 'int_day']\n",
      "Test MAE: 2.663\n",
      "Test MSE: 12.757\n",
      "Test RMSE: 3.572\n",
      "Test R2: -0.035\n",
      "\n",
      "8/62\n",
      "['int_date', 'int_day']\n",
      "Test MAE: 2.573\n",
      "Test MSE: 12.450\n",
      "Test RMSE: 3.529\n",
      "Test R2: -0.010\n",
      "\n",
      "9/62\n",
      "['int_time', 'rain']\n",
      "Test MAE: 2.602\n",
      "Test MSE: 12.596\n",
      "Test RMSE: 3.549\n",
      "Test R2: -0.022\n",
      "\n",
      "10/62\n",
      "['int_date', 'rain']\n",
      "Test MAE: 2.480\n",
      "Test MSE: 12.498\n",
      "Test RMSE: 3.535\n",
      "Test R2: -0.014\n",
      "\n",
      "11/62\n",
      "['int_day', 'rain']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0faf6ae7f703>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_powersets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../datasets/bss/dublin/reorg_plus_weather/station_30.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# # Trying to predict 2020 data with older training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# test_powersets(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-7bc262c12887>\u001b[0m in \u001b[0;36mtest_powersets\u001b[1;34m(start_position, file, train_start_date, train_end_date, test_start_date, test_end_date)\u001b[0m\n\u001b[0;32m     42\u001b[0m                         verbose=0)\n\u001b[0;32m     43\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             train_model(file,\n\u001b[0m\u001b[0;32m     45\u001b[0m                         \u001b[0mcols_to_use\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                         verbose=0)\n",
      "\u001b[1;32m<ipython-input-13-c643eb8b797b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(file_location, train_start_date, train_end_date, test_start_date, test_end_date, cols_to_use, verbose)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     history = model.fit(train_X, train_y,\n\u001b[0m\u001b[0;32m     62\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thest\\documents\\github\\fyp-bss-rebalancing\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thest\\documents\\github\\fyp-bss-rebalancing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thest\\documents\\github\\fyp-bss-rebalancing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thest\\documents\\github\\fyp-bss-rebalancing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thest\\documents\\github\\fyp-bss-rebalancing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\thest\\documents\\github\\fyp-bss-rebalancing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thest\\documents\\github\\fyp-bss-rebalancing\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_powersets(file='../datasets/bss/dublin/reorg_plus_weather/station_30.csv')\n",
    "\n",
    "\n",
    "# # Trying to predict 2020 data with older training set\n",
    "# test_powersets(\n",
    "#                file='../datasets/bss/dublin/reorg_plus_weather/station_4.csv',\n",
    "#                train_start_date=datetime(year=2018, month=8, day=1),\n",
    "#                train_end_date=datetime(year=2020, month=1, day=31),\n",
    "#                test_start_date=datetime(year=2020, month=4, day=1),\n",
    "#                test_end_date=datetime(year=2020, month=12, day=1),\n",
    "#                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_performance_deltas(file='../datasets/bss/dublin/feature_optimization_stats/station_2.csv'):\n",
    "    attr_list = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "    ]\n",
    "    \n",
    "    stats_list = [\n",
    "        'mae',\n",
    "        'mse',\n",
    "        'rmse',\n",
    "        'r2'\n",
    "    ]\n",
    "    \n",
    "    stats = read_csv(file)\n",
    "    \n",
    "    col_names = deepcopy(stats_list)\n",
    "    col_names.insert(0, 'feature')\n",
    "    destination_directory = '../datasets/bss/dublin/feature_optimization_deltas/'\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    destination_file = destination_directory + file.split('/')[-1]\n",
    "\n",
    "    with open(destination_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([col_names])\n",
    "    \n",
    "    with open(destination_file, 'a', newline='') as csvfile:\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=col_names)\n",
    "        \n",
    "        for attr in attr_list:\n",
    "            stats_with_attr = stats[stats[attr] == True]\n",
    "            stats_without_attr = stats[stats[attr] == False]\n",
    "            print(\"--- \" + attr + \" ---\")\n",
    "            line_dict = {}\n",
    "            line_dict['feature'] = attr\n",
    "            for stat in stats_list:\n",
    "                average_with = stats_with_attr[stat].sum() / len(stats_with_attr)\n",
    "                average_without = stats_without_attr[stat].sum() / len(stats_without_attr)\n",
    "                print(stat.upper())\n",
    "                print(\"Average with: \" + str(average_with))\n",
    "                print(\"Average without: \" + str(average_without))\n",
    "                print(\"Average delta (with-without): \" + str(average_with - average_without))\n",
    "                line_dict[stat] = (average_with - average_without)\n",
    "                print()\n",
    "            \n",
    "            writer.writerow(line_dict)\n",
    "        \n",
    "feature_performance_deltas(file='../datasets/bss/dublin/feature_optimization_stats/station_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-darkness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
