{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominican-constitution",
   "metadata": {},
   "source": [
    "# LSTM Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fluid-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_date(df, date):\n",
    "    # print(date)\n",
    "    x = df.index[df['DATE'] == str(date).split(' ')[0]].tolist()\n",
    "    if len(x) == 0:\n",
    "        print(\"Date: \" + str(date) + \" not found in dataset\")\n",
    "        if date.year <= 2018:\n",
    "            print(\"Assuming before start of dataset, returning 0\")\n",
    "            return 0\n",
    "        elif date.year >= 2020:\n",
    "            print(\"Assuming after end of dataset, returning end\")\n",
    "            return (len(df) - 1)\n",
    "            \n",
    "\n",
    "    return x[0]\n",
    "\n",
    "def get_data_split(\n",
    "                file_location,\n",
    "                train_start_date=datetime(year=2018, month=8, day=1),\n",
    "                train_end_date=datetime(year=2019, month=7, day=30),\n",
    "                test_start_date=datetime(year=2019, month=8, day=1),\n",
    "                test_end_date=datetime(year=2019, month=12, day=31),\n",
    "                cols_to_use=None\n",
    "                ):\n",
    "    if cols_to_use is None:\n",
    "        cols_to_use = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "        ]\n",
    "\n",
    "    cols_to_use.insert(0, 'AVAILABLE BIKES')\n",
    "    cols_to_use.insert(0, 'TIME')\n",
    "    # load dataset\n",
    "    dataset = read_csv(file_location, usecols=cols_to_use)\n",
    "    dataset['DATE'] = dataset['TIME'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "    if 'rain' in cols_to_use:\n",
    "        dataset = dataset[dataset['rain'].str.strip().astype(bool)]\n",
    "\n",
    "    train_start_index = (get_index_of_date(dataset, train_start_date))\n",
    "    train_end_index = (get_index_of_date(dataset, train_end_date))\n",
    "    # print( train_end_index - train_start_index)\n",
    "\n",
    "    test_start_index = (get_index_of_date(dataset, test_start_date))\n",
    "    test_end_index = (get_index_of_date(dataset, test_end_date))\n",
    "    # print(test_end_index - test_start_index)\n",
    "\n",
    "    dataset = dataset.drop(['TIME', 'DATE'], axis=1)\n",
    "#     print(dataset)\n",
    "    # print(dataset.head())\n",
    "    # print(dataset)\n",
    "    values = dataset.values\n",
    "    # print(values.shape)\n",
    "\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # print(values.shape)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "#     print(values)\n",
    "#     print(scaled)\n",
    "    # frame as supervised learning\n",
    "    reframed = scaled\n",
    "\n",
    "    # print(scaled)\n",
    "\n",
    "    # split into train and test sets\n",
    "    # values = reframed.values\n",
    "\n",
    "    train = scaled[train_start_index:train_end_index, :]\n",
    "    test = scaled[test_start_index:test_end_index, :]\n",
    "    # train = values[train_start:train_end, :]\n",
    "    # test = values[test_start:test_end, :]\n",
    "\n",
    "    # split into input and outputs\n",
    "    train_x, train_y = train[:, 1:], train[:, 0]\n",
    "    test_x, test_y = test[:, 1:], test[:, 0]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "    test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
    "    # print(train_X.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, scaler\n",
    "\n",
    "def get_trained_model(train_x, train_y, test_x, test_y, verbose=1):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_x, train_y,\n",
    "                        epochs=150,\n",
    "                        batch_size=72,\n",
    "                        validation_data=(test_x, test_y),\n",
    "                        verbose=verbose,\n",
    "                        shuffle=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-occasions",
   "metadata": {},
   "source": [
    "Generate full models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "absent-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▎                                                                         | 10/110 [00:13<02:18,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████                                                                      | 15/110 [00:20<02:04,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|███████████▊                                                                     | 16/110 [00:21<02:03,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▌                                                                    | 17/110 [00:22<02:01,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▉                                                                   | 19/110 [00:24<01:41,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n",
      "Date: 2019-07-30 00:00:00 not found in dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▋                                                                  | 20/110 [00:25<01:25,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n",
      "Date: 2019-07-30 00:00:00 not found in dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [02:24<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "destination_directory = '../datasets/bss/dublin/ml_models/'\n",
    "scaler_destination_directory = '../datasets/bss/dublin/ml_models/scalers/'\n",
    "if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "        \n",
    "if not os.path.exists(scaler_destination_directory):\n",
    "        os.makedirs(scaler_destination_directory)\n",
    "\n",
    "source_directory = '../datasets/bss/dublin/reorg_plus_weather/'\n",
    "files = [f for f in listdir(source_directory) if isfile(join(source_directory, f))]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for file in tqdm(files):\n",
    "# for file in files:\n",
    "    station = file.split('.')[0]\n",
    "    \n",
    "    try:\n",
    "        train_x, train_y, test_x, test_y, scaler = get_data_split(source_directory + file)\n",
    "    except IndexError as e:\n",
    "#         print(\"File \" + file + \" is causing IndexError issues lol\")\n",
    "        continue\n",
    "    except AttributeError as e:\n",
    "#         print(\"File \" + file + \" is causing AttributeError issues lol\")\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(scaler_destination_directory + station + '.pkl'):\n",
    "        file = open(scaler_destination_directory + station + '.pkl', \"wb\")\n",
    "        pickle.dump(scaler, file)\n",
    "        file.close()\n",
    "        \n",
    "    if not os.path.exists(destination_directory + station +'.h5'):\n",
    "#         print(\"\\n Working on \" + station)\n",
    "        \n",
    "        model = get_trained_model(train_x, train_y, test_x, test_y, verbose=2)       \n",
    "        model.save(destination_directory + station +'.h5')\n",
    "#     else:\n",
    "#         print(station + \" station model already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-gates",
   "metadata": {},
   "source": [
    "Generate simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "enhanced-yellow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                         | 11/110 [00:07<01:09,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▊                                                                     | 16/110 [00:11<01:05,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▌                                                                    | 17/110 [00:12<01:05,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████▎                                                                   | 18/110 [00:12<01:03,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████▉                                                                   | 19/110 [00:12<00:50,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n",
      "Date: 2019-07-30 00:00:00 not found in dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████▋                                                                  | 20/110 [00:13<00:41,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n",
      "Date: 2019-07-30 00:00:00 not found in dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████▌                                           | 51/110 [00:35<00:40,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2019-07-30 00:00:00 not found in dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [01:17<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "destination_directory = '../datasets/bss/dublin/simple_ml_models/'\n",
    "scaler_destination_directory = '../datasets/bss/dublin/simple_ml_models/scalers/'\n",
    "if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "        \n",
    "if not os.path.exists(scaler_destination_directory):\n",
    "        os.makedirs(scaler_destination_directory)\n",
    "\n",
    "source_directory = '../datasets/bss/dublin/reorg_plus_weather/'\n",
    "files = [f for f in listdir(source_directory) if isfile(join(source_directory, f))]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for file in tqdm(files):\n",
    "# for file in files:\n",
    "    station = file.split('.')[0]\n",
    "    \n",
    "    try:\n",
    "        train_x, train_y, test_x, test_y, scaler = get_data_split(source_directory + file,\n",
    "                                                                  cols_to_use=['int_time','int_date','int_day']\n",
    "                                                                 )\n",
    "    except IndexError as e:\n",
    "#         print(\"File \" + file + \" is causing IndexError issues lol\")\n",
    "        continue\n",
    "    except AttributeError as e:\n",
    "#         print(\"File \" + file + \" is causing AttributeError issues lol\")\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(scaler_destination_directory + station + '.pkl'):\n",
    "        file = open(scaler_destination_directory + station + '.pkl', \"wb\")\n",
    "        pickle.dump(scaler, file)\n",
    "        file.close()\n",
    "        \n",
    "    if not os.path.exists(destination_directory + station +'.h5'):\n",
    "#         print(\"\\n Working on \" + station)\n",
    "        \n",
    "        model = get_trained_model(train_x, train_y, test_x, test_y, verbose=2)       \n",
    "        model.save(destination_directory + station +'.h5')\n",
    "#     else:\n",
    "#         print(station + \" station model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "alpine-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "destination_directory = '../datasets/bss/dublin/ml_models/'\n",
    "scaler_destination_directory = '../datasets/bss/dublin/ml_models/scalers/'\n",
    "\n",
    "model = tf.keras.models.load_model(destination_directory + 'station_2.h5')\n",
    "train_x, train_y, test_x, test_y, scaler = get_data_split('../datasets/bss/dublin/reorg_plus_weather/station_2.csv',\n",
    "                                                         cols_to_use=['int_time','int_date','int_day','rain','temp','rhum'])\n",
    "\n",
    "\n",
    "file = open(scaler_destination_directory + 'station_2.pkl', \"rb\")\n",
    "scaler = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "worthy-creek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 3.265\n",
      "Test MSE: 22.100\n",
      "Test RMSE: 4.701\n",
      "Test R2: 0.300496743881306205281589427614\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "\n",
    "yhat = model.predict(test_x)\n",
    "# print(yhat)\n",
    "test_x_reshaped = test_x.reshape((test_x.shape[0], test_x.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_x_reshaped), axis=1)\n",
    "# print(\"-----------\")\n",
    "# print(inv_yhat)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "# print(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_x_reshaped), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 0]\n",
    "# calculate RMSE\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# temp = concatenate((inv_y, inv_yhat))\n",
    "# print(temp)\n",
    "# print(inv_y)\n",
    "# print(inv_yhat)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "mse = mean_squared_error(inv_y, inv_yhat)\n",
    "r2 = r2_score(inv_y, inv_yhat)\n",
    "print('Test MAE: %.3f' % mae)\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test R2: %.30f' % r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-picture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "copyrighted-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "destination_directory = '../datasets/bss/dublin/simple_ml_models/'\n",
    "scaler_destination_directory = '../datasets/bss/dublin/simple_ml_models/scalers/'\n",
    "\n",
    "model = tf.keras.models.load_model(destination_directory + 'station_2.h5')\n",
    "train_x, train_y, test_x, test_y, scaler = get_data_split('../datasets/bss/dublin/reorg_plus_weather/station_2.csv',\n",
    "                                                          cols_to_use=['int_time','int_date','int_day'])\n",
    "\n",
    "file = open(scaler_destination_directory + 'station_2.pkl', \"rb\")\n",
    "scaler = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "structured-sequence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 3.025\n",
      "Test MSE: 18.657\n",
      "Test RMSE: 4.319\n",
      "Test R2: 0.409414553649598778051199587935\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "\n",
    "yhat = model.predict(test_x)\n",
    "test_x_reshaped = test_x.reshape((test_x.shape[0], test_x.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_x_reshaped), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_x_reshaped), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 0]\n",
    "# calculate RMSE\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# temp = concatenate((inv_y, inv_yhat))\n",
    "# print(temp)\n",
    "# print(inv_y)\n",
    "# print(inv_yhat)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "mse = mean_squared_error(inv_y, inv_yhat)\n",
    "r2 = r2_score(inv_y, inv_yhat)\n",
    "print('Test MAE: %.3f' % mae)\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test R2: %.30f' % r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-hybrid",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
