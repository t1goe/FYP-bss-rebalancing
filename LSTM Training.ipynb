{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominican-constitution",
   "metadata": {},
   "source": [
    "# LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mobile-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "# def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "#     n_vars = 1 if type(data) is list else data.shape[1]\n",
    "#     df = DataFrame(data)\n",
    "#     cols, names = list(), list()\n",
    "#     # input sequence (t-n, ... t-1)\n",
    "#     for i in range(n_in, 0, -1):\n",
    "#         cols.append(df.shift(i))\n",
    "#         names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "#     # forecast sequence (t, t+1, ... t+n)\n",
    "#     for i in range(0, n_out):\n",
    "#         cols.append(df.shift(-i))\n",
    "#         if i == 0:\n",
    "#             names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "#         else:\n",
    "#             names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "#     # put it all together\n",
    "#     agg = concat(cols, axis=1)\n",
    "#     agg.columns = names\n",
    "#     # drop rows with NaN values\n",
    "#     if dropnan:\n",
    "#         agg.dropna(inplace=True)\n",
    "#     return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "further-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5226481  0.58082193 0.5       ]\n",
      " [0.         0.5261324  0.58082193 0.5       ]\n",
      " [0.         0.5296167  0.58082193 0.5       ]\n",
      " ...\n",
      " [0.05       0.9930314  0.         0.8333334 ]\n",
      " [0.05       0.9965157  0.         0.8333334 ]\n",
      " [0.05       1.         0.         0.8333334 ]]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('datasets/bss/dublin/reorg/station_2.csv')\n",
    "\n",
    "dataset = dataset.drop('TIME', axis=1)\n",
    "# print(dataset.head())\n",
    "values = dataset.values\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = scaled\n",
    "\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "genetic-abuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 1, 3) (8760,) (8927, 1, 3) (8927,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "# values = reframed.values\n",
    "\n",
    "train_start = 0\n",
    "train_end = 8760\n",
    "\n",
    "test_start = 99144\n",
    "test_end = 108071\n",
    "\n",
    "n_train_hours = 365 * 24\n",
    "train = scaled[train_start:train_end, :]\n",
    "test = scaled[test_start:test_end, :]\n",
    "# train = values[train_start:train_end, :]\n",
    "# test = values[test_start:test_end, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, 1:], train[:, 1]\n",
    "test_X, test_y = test[:, 1:], test[:, 1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "responsible-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "122/122 [==============================] - 5s 11ms/step - loss: 0.4197 - val_loss: 0.1791\n",
      "Epoch 2/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1645 - val_loss: 0.1138\n",
      "Epoch 3/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0922 - val_loss: 0.0229\n",
      "Epoch 4/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0173 - val_loss: 0.0084\n",
      "Epoch 5/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0094 - val_loss: 0.0076\n",
      "Epoch 6/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0083 - val_loss: 0.0067\n",
      "Epoch 7/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0059\n",
      "Epoch 8/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0078 - val_loss: 0.0051\n",
      "Epoch 9/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0065 - val_loss: 0.0060\n",
      "Epoch 10/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0063 - val_loss: 0.0043\n",
      "Epoch 11/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0052 - val_loss: 0.0067\n",
      "Epoch 12/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0071 - val_loss: 0.0050\n",
      "Epoch 13/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 14/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0057 - val_loss: 0.0042\n",
      "Epoch 15/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0051 - val_loss: 0.0116\n",
      "Epoch 16/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0088 - val_loss: 0.0050\n",
      "Epoch 17/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0094 - val_loss: 0.0053\n",
      "Epoch 18/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0048 - val_loss: 0.0037\n",
      "Epoch 19/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0075\n",
      "Epoch 20/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0090 - val_loss: 0.0057\n",
      "Epoch 21/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0029\n",
      "Epoch 22/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0066 - val_loss: 0.0046\n",
      "Epoch 23/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 24/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.0037\n",
      "Epoch 25/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0040 - val_loss: 0.0030\n",
      "Epoch 26/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0059 - val_loss: 0.0049\n",
      "Epoch 27/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0109\n",
      "Epoch 28/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0089 - val_loss: 0.0050\n",
      "Epoch 29/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0035 - val_loss: 0.0082\n",
      "Epoch 30/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 31/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0060 - val_loss: 0.0040\n",
      "Epoch 32/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 33/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0052 - val_loss: 0.0101\n",
      "Epoch 34/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0065 - val_loss: 0.0059\n",
      "Epoch 35/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.0049\n",
      "Epoch 36/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0079 - val_loss: 0.0062\n",
      "Epoch 37/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 38/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0034\n",
      "Epoch 39/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0032 - val_loss: 0.0052\n",
      "Epoch 40/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0052 - val_loss: 0.0032\n",
      "Epoch 41/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0037 - val_loss: 0.0029\n",
      "Epoch 42/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0041 - val_loss: 0.0023\n",
      "Epoch 43/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0042 - val_loss: 0.0075\n",
      "Epoch 44/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0049 - val_loss: 0.0047\n",
      "Epoch 45/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 46/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0031 - val_loss: 0.0039\n",
      "Epoch 47/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.0048\n",
      "Epoch 48/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0058 - val_loss: 0.0027\n",
      "Epoch 49/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 0.0048\n",
      "Epoch 50/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0039 - val_loss: 0.0085\n",
      "Epoch 51/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0060 - val_loss: 0.0035\n",
      "Epoch 52/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0061 - val_loss: 0.0079\n",
      "Epoch 53/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0062 - val_loss: 0.0135\n",
      "Epoch 54/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0089 - val_loss: 0.0025\n",
      "Epoch 55/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0021\n",
      "Epoch 56/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0060\n",
      "Epoch 57/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 58/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 59/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.0041\n",
      "Epoch 60/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0037 - val_loss: 0.0082\n",
      "Epoch 61/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0073 - val_loss: 0.0037\n",
      "Epoch 62/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0040 - val_loss: 0.0052\n",
      "Epoch 63/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 64/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 65/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0067 - val_loss: 0.0038\n",
      "Epoch 66/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0033 - val_loss: 0.0046\n",
      "Epoch 67/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0045 - val_loss: 0.0078\n",
      "Epoch 68/150\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.0052 - val_loss: 0.0034\n",
      "Epoch 69/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0028 - val_loss: 0.0025\n",
      "Epoch 70/150\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0033 - val_loss: 0.0071\n",
      "Epoch 71/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 72/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 73/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0050 - val_loss: 0.0045\n",
      "Epoch 74/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0063 - val_loss: 0.0021\n",
      "Epoch 75/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.0065\n",
      "Epoch 76/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0038 - val_loss: 0.0082\n",
      "Epoch 77/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.0021\n",
      "Epoch 78/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 79/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.0027\n",
      "Epoch 80/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0045 - val_loss: 0.0098\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 82/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0050 - val_loss: 0.0060\n",
      "Epoch 83/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0039 - val_loss: 0.0082\n",
      "Epoch 84/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0052 - val_loss: 0.0028\n",
      "Epoch 85/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0037\n",
      "Epoch 86/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0048 - val_loss: 0.0027\n",
      "Epoch 87/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 88/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0066\n",
      "Epoch 89/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0069 - val_loss: 0.0037\n",
      "Epoch 90/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.0086\n",
      "Epoch 91/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0054 - val_loss: 0.0030\n",
      "Epoch 92/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.0063\n",
      "Epoch 93/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0054 - val_loss: 0.0019\n",
      "Epoch 94/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.0068\n",
      "Epoch 95/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0053 - val_loss: 0.0041\n",
      "Epoch 96/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.0022\n",
      "Epoch 97/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 98/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.0051\n",
      "Epoch 99/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0052\n",
      "Epoch 100/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 101/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 102/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0038 - val_loss: 0.0046\n",
      "Epoch 103/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 104/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0044 - val_loss: 0.0015\n",
      "Epoch 105/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 106/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 107/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0078 - val_loss: 0.0049\n",
      "Epoch 108/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0041 - val_loss: 0.0019\n",
      "Epoch 109/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0052 - val_loss: 0.0043\n",
      "Epoch 110/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 111/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0048 - val_loss: 0.0028\n",
      "Epoch 112/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.0055\n",
      "Epoch 113/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0038 - val_loss: 0.0069\n",
      "Epoch 114/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 115/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 116/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0057 - val_loss: 0.0042\n",
      "Epoch 117/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0048\n",
      "Epoch 118/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.0053\n",
      "Epoch 119/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 120/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.0088\n",
      "Epoch 121/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0064 - val_loss: 0.0041\n",
      "Epoch 122/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 123/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0066 - val_loss: 0.0018\n",
      "Epoch 124/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 125/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.0063\n",
      "Epoch 126/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.0023\n",
      "Epoch 127/150\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 128/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0047\n",
      "Epoch 129/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0032 - val_loss: 0.0109\n",
      "Epoch 130/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0075 - val_loss: 0.0093\n",
      "Epoch 131/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0056 - val_loss: 0.0084\n",
      "Epoch 132/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0072 - val_loss: 0.0030\n",
      "Epoch 133/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 134/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.0046\n",
      "Epoch 135/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 136/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0029\n",
      "Epoch 137/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.0055\n",
      "Epoch 138/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0039 - val_loss: 0.0067\n",
      "Epoch 139/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0107 - val_loss: 0.0045\n",
      "Epoch 140/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0046 - val_loss: 0.0019\n",
      "Epoch 141/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 142/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0048 - val_loss: 0.0080\n",
      "Epoch 143/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.0058\n",
      "Epoch 144/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.0083\n",
      "Epoch 145/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 146/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0057 - val_loss: 0.0020\n",
      "Epoch 147/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 148/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0051 - val_loss: 0.0031\n",
      "Epoch 149/150\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0032 - val_loss: 0.0051\n",
      "Epoch 150/150\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.0025\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtcklEQVR4nO3deXhU5fn/8fc9S/YQQhIQCLIjq4JEqtVad8EF3BXrV9va0n6rrf5srVitW5ev1VatFbe2dHNBRa2oKCiCSt0IiLJDQIQEgZAQsiez3L8/zglMQoABApOe3K/rypWZs0zuOZn5zDPPec45oqoYY4zxLl+iCzDGGHNoWdAbY4zHWdAbY4zHWdAbY4zHWdAbY4zHBRJdQEu5ubnap0+fRJdhjDH/VRYuXLhNVfNam9fugr5Pnz4UFhYmugxjjPmvIiJf7mmedd0YY4zHWdAbY4zHWdAbY4zHtbs+emOMORChUIji4mLq6+sTXcohlZKSQn5+PsFgMO51LOiNMZ5QXFxMZmYmffr0QUQSXc4hoaqUlZVRXFxM3759417Pum6MMZ5QX19PTk6OZ0MeQETIycnZ728tFvTGGM/wcsg3OZDnGFfQi8hYEVklIkUiMrmV+T8UkSUislhE5ovI0Jh5t7rrrRKRs/e7wjjVNIR5YPYqFm+sOFR/whhj/ivtM+hFxA9MAcYBQ4GJsUHuekZVR6jqSOA+4AF33aHAFcAwYCzwqPt4ba4+FOHhd4r4zILeGJMAFRUVPProo/u93jnnnENFRUXbFxQjnhb9GKBIVdepaiMwDZgQu4CqVsbcTQearmYyAZimqg2q+gVQ5D5emwv4nacSikQPxcMbY8xe7Snow+HwXtebOXMmnTt3PkRVOeIZddMT2Bhzvxj4WsuFROQ64CYgCTgtZt2PWqzbs5V1JwGTAI488sh46t5N0s6gtytmGWMOv8mTJ7N27VpGjhxJMBgkJSWF7OxsVq5cyerVq7ngggvYuHEj9fX13HDDDUyaNAnYddqX6upqxo0bx0knncQHH3xAz549eeWVV0hNTT3o2tpseKWqTgGmiMiVwO3ANfux7pPAkwAFBQUHlNQBv7ODImwtemM6vLtfXcbyTZX7XnA/DO3RiTvPH7bH+ffeey9Lly5l8eLFzJs3j3PPPZelS5fuHAY5depUunTpQl1dHccddxwXX3wxOTk5zR5jzZo1PPvss/z5z3/msssu48UXX+Sqq6466Nrj6bopAXrF3M93p+3JNOCCA1z3gAV8TtCHotaiN8Yk3pgxY5qNdX/44Yc55phjOP7449m4cSNr1qzZbZ2+ffsycuRIAEaPHs369evbpJZ4WvQLgIEi0hcnpK8AroxdQEQGqmpT1ecCTbdnAM+IyANAD2Ag8ElbFN6SiBDwibXojTF7bXkfLunp6Ttvz5s3j7fffpsPP/yQtLQ0TjnllFbHwicnJ++87ff7qaura5Na9hn0qhoWkeuBWYAfmKqqy0TkHqBQVWcA14vIGUAI2I7bbeMu9zywHAgD16lqpE0qb0XAL4StRW+MSYDMzEyqqqpanbdjxw6ys7NJS0tj5cqVfPTRR60ud6jE1UevqjOBmS2m3RFz+4a9rPsb4DcHWuD+CPp8NurGGJMQOTk5nHjiiQwfPpzU1FS6deu2c97YsWN5/PHHGTJkCEcddRTHH3/8Ya3NU+e6CQYs6I0xifPMM8+0Oj05OZk33nij1XlN/fC5ubksXbp05/Sf/exnbVaXp06B4PTRW9eNMcbE8lTQB/0+G0dvjDEteCronZ2x1nVjjDGxvBX01nVjjDG78VTQB/0+Gm1nrDHGNOO5oLcDpowxpjlPBb0dMGWMSZQDPU0xwEMPPURtbW0bV7SLp4LeDpgyxiRKew56Tx0wFfALjWELemPM4Rd7muIzzzyTrl278vzzz9PQ0MCFF17I3XffTU1NDZdddhnFxcVEIhF++ctfsmXLFjZt2sSpp55Kbm4uc+fObfPaPBb0PmoaD9mpdIwx/y3emAybl7TtYx4xAsbdu8fZsacpnj17NtOnT+eTTz5BVRk/fjzvvfcepaWl9OjRg9dffx1wzoGTlZXFAw88wNy5c8nNzW3bml2e6rpJ8gsha9EbYxJs9uzZzJ49m1GjRnHssceycuVK1qxZw4gRI3jrrbe45ZZbeP/998nKyjos9XirRe/z2QFTxpi9trwPB1Xl1ltv5Qc/+MFu8xYtWsTMmTO5/fbbOf3007njjjtaeYS25akWfcBvB0wZYxIj9jTFZ599NlOnTqW6uhqAkpIStm7dyqZNm0hLS+Oqq67i5ptvZtGiRbuteyh4qkUf9PsIWYveGJMAsacpHjduHFdeeSUnnHACABkZGTz11FMUFRVx88034/P5CAaDPPbYYwBMmjSJsWPH0qNHj0OyM1ZU21cLuKCgQAsLCw9o3Ztf+Iz5Rdv48NbT27gqY0x7t2LFCoYMGZLoMg6L1p6riCxU1YLWlvdU142dj94YY3bnraD3iZ2m2BhjWvBU0AfsXDfGdGjtrSv6UDiQ5+ixoBdCdq4bYzqklJQUysrKPB32qkpZWRkpKSn7tZ63Rt34rEVvTEeVn59PcXExpaWliS7lkEpJSSE/P3+/1vFU0Af8QlQhElX8Pkl0OcaYwygYDNK3b99El9EuearrJuh3no6NvDHGmF08FvROK97OSW+MMbvEFfQiMlZEVolIkYhMbmX+TSKyXEQ+F5E5ItI7Zl5ERBa7PzPasviWAj7n6Vg/vTHG7LLPPnoR8QNTgDOBYmCBiMxQ1eUxi30KFKhqrYj8L3AfcLk7r05VR7Zt2a1ratHbWHpjjNklnhb9GKBIVdepaiMwDZgQu4CqzlXVpsujfATs3y7hNhJw++jtDJbGGLNLPEHfE9gYc7/YnbYn1wJvxNxPEZFCEflIRC5obQURmeQuU3gwQ6N27owNW4veGGOatOnwShG5CigAvhkzubeqlohIP+AdEVmiqmtj11PVJ4EnwTmp2YH+/Z1dN9aiN8aYneJp0ZcAvWLu57vTmhGRM4DbgPGq2tA0XVVL3N/rgHnAqIOod6927Yy1Fr0xxjSJJ+gXAANFpK+IJAFXAM1Gz4jIKOAJnJDfGjM9W0SS3du5wIlA7E7cNhXYuTPWWvTGGNNkn103qhoWkeuBWYAfmKqqy0TkHqBQVWcA9wMZwAsiArBBVccDQ4AnRCSK86Fyb4vROm3KxtEbY8zu4uqjV9WZwMwW0+6IuX3GHtb7ABhxMAXuDxtHb4wxu/PYkbHO02m0oDfGmJ08FvRu143tjDXGmJ08FfR2wJQxxuzOW0Hvs1MgGGNMS54K+qY+euu6McaYXTwW9DaO3hhjWvJY0NuFR4wxpiVPBX3ADpgyxpjdeCvo7YApY4zZjaeC3i48Yowxu/NU0Ns4emOM2Z2ngt5a9MYYsztvBb3PRt0YY0xLngp6n0/wiR0wZYwxsTwV9OD009ulBI0xZhfPBX3QJ9aiN8aYGN4L+oDP+uiNMSaG54I+4PPZqBtjjInhuaAP+sWOjDXGmBieC/qAX+xcN8YYE8NzQR/0WR+9McbE8lzQB/w26sYYY2J5LuiDfmvRG2NMLM8FvXPAlLXojTGmSVxBLyJjRWSViBSJyORW5t8kIstF5HMRmSMivWPmXSMia9yfa9qy+NY4B0xZi94YY5rsM+hFxA9MAcYBQ4GJIjK0xWKfAgWqejQwHbjPXbcLcCfwNWAMcKeIZLdd+buzPnpjjGkunhb9GKBIVdepaiMwDZgQu4CqzlXVWvfuR0C+e/ts4C1VLVfV7cBbwNi2Kb11QTvXjTHGNBNP0PcENsbcL3an7cm1wBv7s66ITBKRQhEpLC0tjaOkPbOdscYY01yb7owVkauAAuD+/VlPVZ9U1QJVLcjLyzuoGgJ2UjNjjGkmnqAvAXrF3M93pzUjImcAtwHjVbVhf9ZtS9aiN8aY5uIJ+gXAQBHpKyJJwBXAjNgFRGQU8AROyG+NmTULOEtEst2dsGe50w4ZOwWCMcY0F9jXAqoaFpHrcQLaD0xV1WUicg9QqKozcLpqMoAXRARgg6qOV9VyEfkVzocFwD2qWn5Inokr4PNZ140xxsTYZ9ADqOpMYGaLaXfE3D5jL+tOBaYeaIH7K+gX67oxxpgYnjsy1vrojTGmOc8FvR0wZYwxzXku6O2AKWOMac5zQW/j6I0xpjnvBb3fRziqqFrYG2MMeDDok/wCYBcIN8YYl3eCPhqFqi2kah0AYeunN8YYwEtBX1MKfxjE4K3OcH9r0RtjjMM7QZ+c4fyKui16G0tvjDGAl4I+kArIrqC3890YYwzgpaD3+SApnSQ36O3oWGOMcXgn6MENeudCV9ZHb4wxDu8FfcQJeuujN8YYh8eCPoNgxFr0xhgTy3NBHwjbOHpjjInlsaBPJ2AtemOMacZ7QR+uAWzUjTHGNPFW0CdnEAg37Yy1Fr0xxoDXgj4pA19Ti9766I0xBvBc0KfjD9UCai16Y4xxeS7oRSMkE7Jx9MYY4/JY0GcCkE49ITvXjTHGAJ4L+nQA0qSeUNha9MYYAx4N+nTq7YApY4xxxRX0IjJWRFaJSJGITG5l/skiskhEwiJySYt5ERFZ7P7MaKvCW5XknJM+nXo7YMoYY1yBfS0gIn5gCnAmUAwsEJEZqro8ZrENwLeBn7XyEHWqOvLgS42De/GRdKm3nbHGGOPaZ9ADY4AiVV0HICLTgAnAzqBX1fXuvMSma1MfPfV24RFjjHHF03XTE9gYc7/YnRavFBEpFJGPROSC1hYQkUnuMoWlpaX78dAtxPTRN9jOWGOMAQ7PztjeqloAXAk8JCL9Wy6gqk+qaoGqFuTl5R34X2rqo5cG6kORA38cY4zxkHiCvgToFXM/350WF1UtcX+vA+YBo/ajvv3jBn1nfwN1jRb0xhgD8QX9AmCgiPQVkSTgCiCu0TMiki0iye7tXOBEYvr221zQuUB4lr+BOmvRG2MMEEfQq2oYuB6YBawAnlfVZSJyj4iMBxCR40SkGLgUeEJElrmrDwEKReQzYC5wb4vROm1LBJIyyPQ1WtAbY4wrnlE3qOpMYGaLaXfE3F6A06XTcr0PgBEHWeP+SUons8H66I0xpom3jowFSM4gQ+qtj94YY1zeC/qkdNKl3rpujDHG5cGgzyCNeupCNo7eGGPAk0GfThr11FvXjTHGAJ4M+gxStc66bowxxuXBoE8nOWpBb4wxTeIaXvlfJSmD5Ggd9RELemOMAY+26JOiddSFwomuxBhj2gVPBr2PKP5oIyE7J70xxngw6JOdC4RnYP30xhgDXgz6mAuE2xBLY4zxcNCnY2ewNMYY8HDQO0fHWtAbY4wHg97to5c6O7GZMcbgyaBvatFb140xxoCHgz6dejsnvTHG4Mmgd64bmyb11DXaOHpjjPFg0O9q0VvXjTHGeDHoAykApIhdN9YYY8CLQe/zof5kUgjZAVPGGIMXgx4gkEwy1qI3xhjwaNBLMJU0X8iC3hhj8GjQE0hxgt66bowxJr6gF5GxIrJKRIpEZHIr808WkUUiEhaRS1rMu0ZE1rg/17RV4XsVTCVdQjaO3hhjiCPoRcQPTAHGAUOBiSIytMViG4BvA8+0WLcLcCfwNWAMcKeIZB982fsQSCHVum6MMQaIr0U/BihS1XWq2ghMAybELqCq61X1c6DlEUpnA2+parmqbgfeAsa2Qd17F0ghVazrxhhjIL6g7wlsjLlf7E6LR1zrisgkESkUkcLS0tI4H3ovgimkYC16Y4yBdrIzVlWfVNUCVS3Iy8s7+AcMpJIijdZHb4wxxBf0JUCvmPv57rR4HMy6By6YQrLaOHpjjIH4gn4BMFBE+opIEnAFMCPOx58FnCUi2e5O2LPcaYdWIMU5YMr66I0xZt9Br6ph4HqcgF4BPK+qy0TkHhEZDyAix4lIMXAp8ISILHPXLQd+hfNhsQC4x512aAVSCGoj9SE7e6UxxgTiWUhVZwIzW0y7I+b2ApxumdbWnQpMPYga918wlaDahUeMMQbiDPr/OoEUC3pjjHG1i1E3bS6Yil8jNIYaUdVEV2OMMQnlzaAPJAOQTIiGsPXTG2M6No8GfSoAKTRSayNvjDEdnDeDPuheZcrOSW+MMR4N+qYWvdhYemOM8WjQO330KdhpEIwxxptBH3Ra9Ml2YjNjjPFo0AfcPnrrujHGGI8G/c4Wve2MNcYYbwZ9U4seu5ygMcZ4OujtDJbGGOPVoG8aRy+2M9YYY7wZ9DFHxlrQG2M6Om8GvduiT7VRN8YY49Ggd/voM/1hqurDCS7GGGMSy5tB7/ODL0hmIEJ1gwW9MaZj8+aFRwCCqWRoiGpr0RtjOjhvtugBAilk+MJUNYQSXYkxxiSUh1v0KaRbi94YY7zdok/1haiyPnpjTAfn7aCXkI26McZ0eN4N+mAqKdJoXTfGmA7Pu0EfSCFZnSNjQxG7QLgxpuOKK+hFZKyIrBKRIhGZ3Mr8ZBF5zp3/sYj0caf3EZE6EVns/jzexvXvWSCFJJwRNzXWT2+M6cD2OepGRPzAFOBMoBhYICIzVHV5zGLXAttVdYCIXAH8DrjcnbdWVUe2bdlxCKYQ1AYAqurDdE5LOuwlGGNMexBPi34MUKSq61S1EZgGTGixzATgH+7t6cDpIiJtV+YBCKQSjO4KemOM6ajiCfqewMaY+8XutFaXUdUwsAPIcef1FZFPReRdEflGa39ARCaJSKGIFJaWlu7XE9ijYAoBN+jtNAjGmI7sUO+M/Qo4UlVHATcBz4hIp5YLqeqTqlqgqgV5eXlt85cDqfgjTUFvR8caYzqueIK+BOgVcz/fndbqMiISALKAMlVtUNUyAFVdCKwFBh1s0XEJJOOLWNeNMcbEE/QLgIEi0ldEkoArgBktlpkBXOPevgR4R1VVRPLcnbmISD9gILCubUrfh2AqEm3ER9SC3hjToe1z1I2qhkXkemAW4AemquoyEbkHKFTVGcBfgX+JSBFQjvNhAHAycI+IhIAo8ENVLT8UT2Q3MdeNtT56Y0xHFtdJzVR1JjCzxbQ7Ym7XA5e2st6LwIsHWeOBCTqXE0z3haiqtz56Y0zH5ekjYwGyk9ROg2CM6dA8H/RdkiJ2BktjTIfm3aB3LxDeJTliO2ONMR2ad4M+4PTRdw5GrOvGGNOheTfo3RZ9VtAuEG6M6di8G/RuH32nYMRG3RhjOjTvB30gbC16Y0yH5t2gd8fRZ/rCtjPWGNOheTfo3RZ9hj9EQzhKY9iuMmWM6Zi8G/RNR8b6nda8dd8YYzoq7wZ9IBmANJ8T8LZD1hjTUXk46J0WfSp2qmJjTMfm3aD3ByCzO51rNwDWdWOM6bi8G/QAPUfTqfwzwFr0xpiOy9tBn19AcuV6OlNllxM0xnRY3g76ngUAjPQV2flujDEdlreDvscoVHyM8q2l0oLeGNNBeTvokzMgbzCjfEVsraxPdDWmyYK/wLp5ia7CmA7D20EPSH4BowNf8OwnG1i1uaptH7y2HNbObdvH9LqShfD6T2HOrxJdiTEdhueDnp4FpEerGJZcyk3PL27bUyHM/iX86wLYvLTtHjNWYy1MOR4+nBL/OltXwqbFh6aeg6UKs253bpcshJqyxNZzKNRXQnVpoqswTd67Hx4/CZZMh2jHPQ2K94M+39kh+5ujt7Fs0w6u+svHPL9gI+U1jTsXiUQVVd3jQ6wrraZwfXnzibXlsOQFAHT+gyxYX87v3lzJz6d/xrbqhrapvfCvULoC5t3r/D2A+h0Q2cMIolA9PHUR/G0clK5qmxoOxsYFTr1NVrwKGz6AUf8DKKx9Z/8eLxKC6q1tWuJ+qylztvOePHsF/OEomH5t8wZAuGHX/3B/NNbC589DuHHfy3qFKnz4KGxZfnCPU/kVvPd7KFsLL14LfxoFr90E6/9zUA+7x2NyWn6QlK2FxpqD+lttxftBnzcYUrswdPE9LMu6ke+W3svHLz/Cub+exvl/ms8FU/7D0DveZMxv53DXjGXMX7ONSvd0CfU1lfzlzU8Y+9D7XPbEh7z2+aZdj7vonxBpYPMRpxBd+hI/e+Jl/vzeOv796SYmPvkRpVUNTthOvxbWz29W0uotVVzy2Ad8uHbPLdpoXSXMfxC6jUAbqvh82l0sW/g+PHQ0/GO8ExzAjroQtY3uC69wKlSWgPjghe9AqO7gt9/29fDi96F09f6tt2Q6/PUMeOHbzhu3oRpm3w55Q+DcByAtB4rejv/xohF4+lJ4eBSUr9u/Wla9AX8aDcWFuz/m4mfia4HXlsObv3BC/KXvt75M8UL48j/Q++uwehY8eQp8/ITz7eXRE5wa9vUtRhXqKnbdf+sO5+99/Nie1ylft+cP/7awowRdP5/nCzfyi5eXUFIR5+uqoQrWvbv/LeklL8CsW+G5qw7uNTz/QWe7/HA+XPxXyB0Enz8H/zhv/z9EolGo2MC0j7/k6Ltm8ewnG5rPr9gIvx/gvN5ry2H+Q/BIATz+jUP3jX8/yN5asolQUFCghYWF+15wf1RugjWzYd276BfvIbXbANjk78mS5FHsOOLrVFXX0HnzfMqj6TwRPp8+we08KA/SVSqYnz2BN/ynUrx5KxOOG8Andd25efWVbIjk8OPG6/lPyo18lT+O3FN+yBel1Vw6M8rgrBDT5DaSq5wXxDsyhm2n3Mfpo4dywaP/YWN5HZ1SArx83Yn0z8sAQFV5f802pi3YwICVT3CT/zmqr55N0asPMLh8DnWSQqfUJPx1ZXDMRJYU3Mu3/76AjJQAz14zgu5//xpf+HvzcurF/HTrL2joczrJX/8BEYXlrz1MtKqUDX0vZchZ32VA9xxn24TqYEcJ5A7YfbupwlMXw9o5kJoNVz4Pvca0uokjUeW91aUc2zubrNJF8I/zISULarbyeO5t9Ksq5IyG2TyY/yCl2aP5Vsmv6V/1Cb/s/xIffLGdXtlpPNrnXXIbiuG470GPkc3/wLv3wdzfgD8J7XEs9d96lVR/BGpKofORe/7fF70Nz06ESCNk94Ufvg/Jmc68+Q/B23dC7lFwzauQ2S3mqStLSyp5Z+VWRuZEOHnuxUjVJug2DDYvge/PhZ7HNv9b06+FNbMp/s5C/l24nnPW3k2/7e6HfGYPqN4CBd+Bc/9AQzjC6s3VNEaiZKUGGdA1A8INVL3wv6SvepnC4b8kt/9I+r1ykXOCPl8QblgM29bA6zfBuN9Bn5Ocndr/nEC090nMOOp3jB7cj15d0nbVFI3C9i+c8Ik0QLfhkNp5z9sLqKhtpCEcpVunFGisJfLEyfjL1vBy5ETuDF1DY7ATFx2bT1rQTzDgY2BaLQPT6xgy8gQCfp/zQfXe/U5jqKHS+X+e83unjo+fgGEXor2+hog4QbyjGKo2Q/ej2by9iq7//Aa+YApUbKBhzPX4z/4VgYov0IyuLC2N0q1TMl07pTiNneevhpTOcMFj4Itpt+4ogYdHwtGXw4RHCEeiNEaipIUr4eFR1OSO4F8DHkK2raT/9vkEjzqLYceeSG5G8s7//+bKetKop9PCKbD4WaSymLcio7mDH1BBFjNv+AZ9c9MBKP3LJWQVz8Mnit8fRMJ1MPBs+OozqK+Ai/5MVb9xpCcF8Pmk+QavqwAR5/1yEERkoaoWtDqvQwR9rGgUti5zWhpfvOt8jQs5X680JRsaKgn5kvFFQ9QldaG2xwl0Wz8DdFerZBudyaWC1wffS+rIizh1zb3Iwqk759d0Gcba8hBH6RfcknonPao+54bgy2yM5vET/+0MDy9lct6HvLm9B+8lf5NzjxtEUqSWvy2HTzZH+X7qXG7iad6LDOenvp+T1VDCvOSfsY0sJmfey/2DV5G74A/M12NYGxhAeSSZY3xfcFr0Qy5suJtt2Uczbsfz3BB4mXRxuhlKtRMNgSzyIxsp1U582OVChgweRv9lD+OrLGZB9rn8ovpyOufk0aNzKtX1YQZun8fkHb/mKd94TqOQrtGtVKT0IJSSx/bUI6kIdqV33XJydyxhJiczufIirshexZ3Rx9DUbB7u8winL/ox/XxfkUEdL6ZdymOB/6GyLsQ3G+Zyv+8RrvLdS6f+Y8gueonf6CNE8OEnyqf+o3km+4cEjxjCVemFDPnkFjb1Oo8P9Ggu3fhr3o6O5mvBdWRGtlPX43jqj74ayR1AaSiFdxYuo2rTas7LXM1RZXNoyOrPZwP+lzEf/5gleeeTcdlj9It+6bS4exyLbl5COKM7G4ZfR4nmULX2E7K2fMxrDccwLXIqTwQf5DT/Z3x62lMMHTmG5EdGsdI/iL/3/T0/OekIcjOCfLT8S05583ReTz2fGyouQwDVKLdkzWFs91qWDL6BwSsfYcCX03jla8/x+MJqqN7CKs1H8TFpSJjvVzxI3vZPWRntxWDfRso0EwmmUjvhr/R4cQKfJI1hZGQpKZFqKoN5/Cj1Ph6uv40MfxipK6c4msPN8lOuvuh8xuTUE3zrNjptmk8wVLnztRlKyWFu35uo7zKE03yfEvnqc6q/WkNjcg49v3E1RTkn852nllLTEOGRK0cx+vN7SF/2FC9GTuaiwHw04wh+m/NbnlmbzAks5UfyAqNlNT5RnpOxrOn7Lb5ffBu5jSWU9DibLl26kLH0KSp6nUFayX9Iijot9A+jw+ieFuXIxrX4ok63VG0wmyUN3SiQVdyQ+QBn1r7OeZE5rJHeDGY9a/wDuKDmVnwpmdx/8QjGrr5jZ/fp+72vY0nf73LuiO50Sw5RP+3bdCp5nxdPfIVPtmfy9ootVNaHOaFfDufWvsLE8ik8Hj6Pq/xzyBCnpiXRvnx+9O2ceMo4fv36cpauWMFfk37PENnA+9ERLNfefC/wJpKWzfdqr2d7XgHf+Xofyj59lWs33MJjgat4tXYo9yQ/w/aep7Jl2PdIDZVz4oIfk121hgsb70K7jeDJ/xlNz86p/KeolK7rXmbQ4t8gwTR04rPU5IwgIzlwQNF20EEvImOBPwJ+4C+qem+L+cnAP4HRQBlwuaqud+fdClwLRICfqOqsvf2tQx70LUVCULLIOTdO95HO1+C5v3Fas+c9CGldYFsRfLWYUHI2O7Z8Sc66fyPhevj26+APOv3Qq2dBaheo3uy0Zrav59V+d3LvppH8vzMHcVGX9YSeuhxfuI6gRKBLf6IVG3e+yJs0BDuRHKqEfqewePRvuf61LZw19Ah+ObKaj8tSufK5DURVuSUwjfOSFpGvmxGNADBdT6Pz5Y9zxtBuFG2t4qE3l7Jj5TySCVFwxqX88NTBVCx7i/I5D9Fvu9NPuTTah0U6iCt9bxP2JbHZ352SaA4lwT6cFn6XBn8mjwz8K1UVpZy89RkyGzaTSzn9ZRNdpJp10SMo0p6c5V9IXTCb1NB2VtCPGyI3sCacx41Da/jJuh8g3YbD9+ZAIMl5ojXb0PsHwPCLkQFnoK/dyJrgYCb7f8YlgfcZX/ksadFqtmsmObKDFdEjubjxLjSYxtNZj3Fs9bu8p8fwcfgoJgbeIV+27favLdNM3o0ew69CV7GdTtwcmMZ1gRms1nxyk6MEInVcLH+gc816/pL0e7Kkdue6Ff4udI6UE+o+muBXC3lAruHhurMRgR/4ZjA5OI0Z0ZM4QxYQJMwWsulOGT/J+xsDjhrG5cf1Yl1pDT974TM2u0N7s6hmXvJNCEonqcWHUp92BNv83civ+ow6TeKRrJ9y5TU/Imv2jWSsepFJkZuZHRrF/wX+zMTAXEo0l7tCVzMl+Efqfel00komNt5GIJjMk8l/JKlxOy9HTuI03yJSCPFK5Oss1gFs1i74ifCTwEuM9O3q+toQzWMj3egrm+gh5WzRbB4Kfo/VacdybPnr3BZ4ir8znsFXP8Txyevhmcud98bRl8FHj6HZfag66hLKtm2lb9E/iCJUk85Nvpt5u3YgoNwZ+CffCczig+gwXu5+E2f4FlBQNoN1DVksivSjLKUPkaROfLPqNU72L2Fx98t5MPg9uic3cEvxdVRFk5hdN4Tv+F5jS5cC/hS5mFFlM7ks8C73hS5niO9LzvF9zD3hq9mh6fw4+G/68BV3h6/mn5GzyUwJcMaQbnTtlMxby7ZQWVPLrJRfkFP3BXQbQXj8FDYvnUv6gilkhUp5MfINxOdjXPISgtF6Zgz8LRtyvk5WapCJvStJ+/d3iZZ/yR2NV9FAkBsDL5Gcmk7GjR+zsrSB37y+nCUlO6gPOY3DXHbwRurtBIJJ/F/jZYxiFf39W+gS2sIA3yY+5Sh6SDmZ0UoeyZ7Mz2+86YCi7KCCXkT8wGrgTKAYWABMVNXlMcv8CDhaVX8oIlcAF6rq5SIyFHgWGAP0AN4GBqm6ydSKwx70h0IkBNu/3L075KvP0bfvRo65HIZfAvUVhFe/TXVjhNpoEkeENuArXQGDxsKwC0EEVXW+4ro+WlfGxvJa+uWlM7xnFskShUiIzbUgPp/zdTvGe6tLqagLMf6YHs2m125azsoVy/hAR1AThu/0q6LrmuecPv7tX8K21aAR+PZM6H3CzvWiUWVHXYjkoI/kaB1b6gNsqaxnWP0ikubcSVn+6Vyz9hQGdc/mR6cOcLokNi6A7D6Qkdd8e/zrIqdbCCCrF0yaB+m5boHl8N7viVRsYFneOSxPP54hPbtw1BGZpEgYKjdRltSDBevLaWhoJHX7CgI1m0mLVDFsYH8yu/VlWegI5qzcRu+cNAYf0YmenQLogj+z+ZN/0616OXcFbkAHjaNfbjrdM3z09m2lh26hW/+RBLJ7OR/Y8+6FfqfQMPEFFm7YwQdFZfTppFw8/3yoK2dpl7OoCOQyqnERaQNOxHfOfc2eYnVDmHWl1XRKCSICkc9eoNvqp0kffAZ06uHsPyhfR3n/CcxLO5vzvj6SpIDPCdPKTaxtzOLVzzZxdh8/Q1Y8zI5R/8tHOzpz4panyXjvHmqHf4vpPX/ONwfl0Tutkejb9yAL/0Z5p8GsO/mP+PIGAVBa1cCG8hq6ZQY5vXEuNTW1vNF4DGR2Z8LInqzavINXXnqW79X/jf6Rdaj4EY2wJDCcTpNeo3fXbOcJbSuCf06AymIYcRmc/xAkOd0XLH0RPn0axv0OzRnAiq+qWLyxgoBA94a1DB91AtkZu16f22samb6wmM9LdrChrIbLjzuSiQNCSOfeTsMr9i0VVXxLnkNe/oHzOsTHhzkXsezoWxmWF+D4dy7Hv20lADXBLqw48Y/kDj+dTqlBslKD+N3ukp3vp02fOvuRTpm8qyuvvpLNL95CbtHzkJpDoOsgGHcfdBva/HVbt93ZB7bOGVqtnfKRS//WrFszGlW2VNXjEyErNUjKlk+dQRKRRupIoTjQi4zcfHb0OJm/1J1CtHobPy2/k9Sgny4/eRd8fvbXwQb9CcBdqnq2e/9Wd4P9X8wys9xlPhSRALAZyAMmxy4bu9ye/p4ngt4LIiHnm0pT8B4K0ajTb12xAfIGOfsBDpPqhjDpSf5mH6KtKl0NWfmQlNZ8esVG583YqUfr6x1q0SismQV9v7l7bTuKIaOb821zP6gqRMPIwr87/5OhFzj7IVpuo6rNzg7GAafvPu9QWz0LQrXQ75Tmr5dQvbMPQPyQ1XPXh8+BUN3384qEYdVMyO4NRxwd33bYtNjZV9RjVOv/m1CdM2ihZYMoTnsL+ng6g3oCG2PuFwNf29MyqhoWkR1Ajjv9oxbr9mylwEnAJIAjj9zLjjVz+PiDhzbkwdl51qm783OYxd0P6raId9O5V9sVcyB8PjhqXOvzsvIP6CFFxPm/j9nDqKImmUc4P4kw6OzWpwdToOuQtvkb8YS2PwBDx+/f47YcYNBSMHXnlfHaWrsYXqmqT6pqgaoW5OUd2KeZMcaY1sUT9CVAbPMl353W6jJu100Wzk7ZeNY1xhhzCMUT9AuAgSLSV0SSgCuAGS2WmQFc496+BHhHnc7/GcAVIpIsIn2BgcAnbVO6McaYeOyzo9Ltc78emIUzvHKqqi4TkXuAQlWdAfwV+JeIFAHlOB8GuMs9DywHwsB1extxY4wxpu11vAOmjDHGg/Y26qZd7Iw1xhhz6FjQG2OMx1nQG2OMx7W7PnoRKQW+PIiHyAV2P/FJ+9Lea2zv9YHV2FasxrbRHmrsraqtHojU7oL+YIlI4Z52SLQX7b3G9l4fWI1txWpsG+29Ruu6McYYj7OgN8YYj/Ni0D+Z6ALi0N5rbO/1gdXYVqzGttGua/RcH70xxpjmvNiiN8YYE8OC3hhjPM4zQS8iY0VklYgUicjkRNcDICK9RGSuiCwXkWUicoM7vYuIvCUia9zfh+/SSnuu1S8in4rIa+79viLysbs9n3PPXJrI+jqLyHQRWSkiK0TkhPa0HUXk/7n/46Ui8qyIpLSHbSgiU0Vkq4gsjZnW6nYTx8NuvZ+LyLEJqu9+9//8uYi8LCKdY+bd6ta3SkT2cBWSQ19jzLyfioiKSK57/7Bvw3h4Iujd69pOAcYBQ4GJ7vVqEy0M/FRVhwLHA9e5dU0G5qjqQGCOez/RbgBWxNz/HfCgqg4AtuNc4D2R/gi8qaqDgWNwam0X21FEegI/AQpUdTjOWV6voH1sw78DY1tM29N2G4dzKvGBOFd8eyxB9b0FDFfVo3GuV30rgPveuQIY5q7zqPveT0SNiEgv4CxgQ8zkRGzDfVPV//of4ARgVsz9W4FbE11XK3W+gnOR9VVAd3dad2BVguvKx3nDnwa8BgjOUX6B1rZvAurLAr7AHTwQM71dbEd2XUqzC86pv18Dzm4v2xDoAyzd13YDngAmtrbc4ayvxbwLgafd283e1zinTj8hEdvQnTYdp9GxHshN5Dbc148nWvS0fl3b3a5Nm0gi0gcYBXwMdFPVr9xZm4FuiarL9RDwcyDq3s8BKlQ17N5P9PbsC5QCf3O7l/4iIum0k+2oqiXA73Fadl8BO4CFtK9tGGtP2609vo++C7zh3m439YnIBKBEVT9rMavd1BjLK0HfrolIBvAicKOqVsbOU+djP2FjXEXkPGCrqi5MVA1xCADHAo+p6iighhbdNIncjm4f9wScD6QeQDqtfNVvjxL9+tsbEbkNp/vz6UTXEktE0oBfAHckupZ4eSXo2+21aUUkiBPyT6vqS+7kLSLS3Z3fHdiaqPqAE4HxIrIemIbTffNHoLN7/V9I/PYsBopV9WP3/nSc4G8v2/EM4AtVLVXVEPASznZtT9sw1p62W7t5H4nIt4HzgG+5H0bQfurrj/Oh/pn7vskHFonIEbSfGpvxStDHc13bw05EBOcyiytU9YGYWbHX2L0Gp+8+IVT1VlXNV9U+ONvtHVX9FjAX5/q/kPgaNwMbReQod9LpOJenbC/bcQNwvIikuf/zpvrazTZsYU/bbQZwtTty5HhgR0wXz2EjImNxuhLHq2ptzKx2cQ1qVV2iql1VtY/7vikGjnVfp+1iG+4m0TsJ2nBnyTk4e+jXArcluh63ppNwvhZ/Dix2f87B6QOfA6wB3ga6JLpWt95TgNfc2/1w3kRFwAtAcoJrGwkUutvy30B2e9qOwN3ASmAp8C8guT1sQ+BZnP0GIZxAunZP2w1nJ/wU9z20BGcUUSLqK8Lp5256zzwes/xtbn2rgHGJ2oYt5q9n187Yw74N4/mxUyAYY4zHeaXrxhhjzB5Y0BtjjMdZ0BtjjMdZ0BtjjMdZ0BtjjMdZ0BtjjMdZ0BtjjMf9f3oDYyOiUfRRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=150, batch_size=72, validation_data=(test_X, test_y), verbose=1,\n",
    "                    shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "palestinian-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.050\n",
      "Test RMSE: 0.070\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "# print(test_X)\n",
    "# print(yhat)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 0]\n",
    "# calculate RMSE\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# temp = concatenate((inv_y, inv_yhat))\n",
    "# print(temp)\n",
    "# print(inv_y)\n",
    "# print(inv_yhat)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "mse = mean_squared_error(inv_y, inv_yhat)\n",
    "print('Test MAE: %.3f' % mae)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "related-whale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [4], [5], [4, 5], [6], [4, 6], [5, 6], [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "attr_list = [\n",
    "    'int_time',\n",
    "    'int_date',\n",
    "    'int_day',\n",
    "    'rain',\n",
    "    'temp',\n",
    "    'rhum'\n",
    "]\n",
    "\n",
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "    \n",
    "print(list(powerset([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
