{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "northern-effort",
   "metadata": {},
   "source": [
    "# LSTM Feature optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-hotel",
   "metadata": {},
   "source": [
    "The purposes of this is to explore optimization for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import keras\n",
    "from datetime import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interested-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_date(df, date):\n",
    "    # print(date)\n",
    "    x = df.index[df['DATE'] == str(date).split(' ')[0]].tolist()\n",
    "    if len(x) == 0:\n",
    "        print(\"Date: \" + str(date) + \" not found in dataset\")\n",
    "        exit(1)\n",
    "\n",
    "    return x[0]\n",
    "\n",
    "\n",
    "def output_stats_to_csv(file_location, cols_used, mae, mse, rmse, r2):\n",
    "    col_names = ['int_time', 'int_date', 'int_day', 'rain', 'temp', 'rhum', 'mae', 'mse', 'rmse', 'r2', 'timestamp']\n",
    "    destination_directory = './datasets/bss/dublin/feature_optimization_stats/'\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    destination_file = destination_directory + file_location.split('/')[-1]\n",
    "\n",
    "    if not os.path.exists(destination_file):\n",
    "\n",
    "        with open(destination_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows([col_names])\n",
    "\n",
    "    with open(destination_file, 'a', newline='') as csvfile:\n",
    "\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=col_names)\n",
    "\n",
    "        writer.writerow({\n",
    "            'int_time': ('int_time' in cols_used),\n",
    "            'int_date': ('int_date' in cols_used),\n",
    "            'int_day': ('int_day' in cols_used),\n",
    "            'rain': ('rain' in cols_used),\n",
    "            'temp': ('temp' in cols_used),\n",
    "            'rhum': ('rhum' in cols_used),\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'timestamp': datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-president",
   "metadata": {},
   "source": [
    "This is a modified version of the model used to more easily use different combonations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mobile-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(file_location,\n",
    "                train_start_date=datetime(year=2018, month=8, day=1),\n",
    "                train_end_date=datetime(year=2019, month=7, day=30),\n",
    "                test_start_date=datetime(year=2019, month=8, day=1),\n",
    "                test_end_date=datetime(year=2019, month=12, day=31),\n",
    "                cols_to_use=None,\n",
    "                verbose=1\n",
    "                ):\n",
    "    if cols_to_use is None:\n",
    "        cols_to_use = ['int_time', 'int_date', 'int_day']\n",
    "\n",
    "    cols_to_use.insert(0, 'AVAILABLE BIKES')\n",
    "    cols_to_use.insert(0, 'TIME')\n",
    "    # load dataset\n",
    "    dataset = read_csv(file_location, usecols=cols_to_use)\n",
    "    dataset['DATE'] = dataset['TIME'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "    if 'rain' in cols_to_use:\n",
    "        dataset = dataset[dataset['rain'].str.strip().astype(bool)]\n",
    "\n",
    "    train_start_index = (get_index_of_date(dataset, train_start_date))\n",
    "    train_end_index = (get_index_of_date(dataset, train_end_date))\n",
    "    # print( train_end_index - train_start_index)\n",
    "\n",
    "    test_start_index = (get_index_of_date(dataset, test_start_date))\n",
    "    test_end_index = (get_index_of_date(dataset, test_end_date))\n",
    "    # print(test_end_index - test_start_index)\n",
    "\n",
    "    dataset = dataset.drop(['TIME', 'DATE'], axis=1)\n",
    "    # print(dataset.head())\n",
    "    # print(dataset)\n",
    "    values = dataset.values\n",
    "    # print(values.shape)\n",
    "\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # print(values.shape)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    # frame as supervised learning\n",
    "    reframed = scaled\n",
    "\n",
    "    # print(scaled)\n",
    "\n",
    "    # split into train and test sets\n",
    "    # values = reframed.values\n",
    "\n",
    "    train = scaled[train_start_index:train_end_index, :]\n",
    "    test = scaled[test_start_index:test_end_index, :]\n",
    "    # train = values[train_start:train_end, :]\n",
    "    # test = values[test_start:test_end, :]\n",
    "\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, 1:], train[:, 0]\n",
    "    test_X, test_y = test[:, 1:], test[:, 0]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    # print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y,\n",
    "                        epochs=150,\n",
    "                        batch_size=72,\n",
    "                        validation_data=(test_X, test_y),\n",
    "                        verbose=verbose,\n",
    "                        shuffle=False)\n",
    "    # plot history\n",
    "    # pyplot.plot(history.history['loss'], label='train')\n",
    "    # pyplot.plot(history.history['val_loss'], label='test')\n",
    "    # pyplot.legend()\n",
    "    # pyplot.show()\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    # print(test_X)\n",
    "    # print(yhat)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "\n",
    "    inv_yhat = concatenate((yhat, test_X), axis=1)\n",
    "    # print(yhat.shape)\n",
    "    # print(test_X.shape)\n",
    "    # print(inv_yhat.shape)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:, 0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:, 0]\n",
    "    # calculate RMSE\n",
    "\n",
    "    # np.set_printoptions(threshold=sys.maxsize)\n",
    "    # temp = concatenate((inv_y, inv_yhat))\n",
    "    # print(temp)\n",
    "    # print(inv_y)\n",
    "    # print(inv_yhat)\n",
    "\n",
    "    # print()\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "    mse = mean_squared_error(inv_y, inv_yhat)\n",
    "    r2 = r2_score(inv_y, inv_yhat)\n",
    "    print('Test MAE: %.3f' % mae)\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print('Test R2: %.30f' % r2)\n",
    "\n",
    "    output_stats_to_csv(file_location, cols_to_use, mae, mse, rmse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unavailable-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "\n",
    "\n",
    "def test_powersets(start_position=0,\n",
    "                   file='./datasets/bss/dublin/reorg_plus_weather/station_2.csv',\n",
    "                   train_start_date=None,\n",
    "                   train_end_date=None,\n",
    "                   test_start_date=None,\n",
    "                   test_end_date=None,\n",
    "                   ):\n",
    "    attr_list = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "    ]\n",
    "\n",
    "    y = list(powerset(attr_list))\n",
    "    # y.sort()\n",
    "    print(len(y))\n",
    "    y = sorted(y, key=len)\n",
    "    y.pop(0)\n",
    "\n",
    "    for x in y[start_position:]:\n",
    "        print(str(start_position) + \"/\" + str(len(y) - 1))\n",
    "        start_position = start_position + 1\n",
    "        print(x)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        if train_start_date is not None and train_end_date is not None and test_start_date is not None and test_end_date is not None:\n",
    "            train_model(file,\n",
    "                        train_start_date=train_start_date,\n",
    "                        train_end_date=train_end_date,\n",
    "                        test_start_date=test_start_date,\n",
    "                        test_end_date=test_end_date,\n",
    "                        cols_to_use=x,\n",
    "                        verbose=0)\n",
    "        else:\n",
    "            train_model(file,\n",
    "                        cols_to_use=x,\n",
    "                        verbose=0)\n",
    "        print()\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "collaborative-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "0/62\n",
      "['int_time']\n",
      "Test MAE: 7.464\n",
      "Test MSE: 77.294\n",
      "Test RMSE: 8.792\n",
      "Test R2: -1.790454134691343845986466476461\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'train_end', 'train_start', 'test_start', 'test_end'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e6cd8eb6b209>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Trying to predict 2020 data with older training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m test_powersets(\n\u001b[0m\u001b[0;32m      6\u001b[0m                \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./datasets/bss/dublin/reorg_plus_weather/station_4.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                \u001b[0mtrain_start_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mday\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-2d27a0281727>\u001b[0m in \u001b[0;36mtest_powersets\u001b[1;34m(start_position, file, train_start_date, train_end_date, test_start_date, test_end_date)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrain_start_date\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_end_date\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtest_start_date\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtest_end_date\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             train_model(file,\n\u001b[0m\u001b[0;32m     37\u001b[0m                         \u001b[0mtrain_start_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_start_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                         \u001b[0mtrain_end_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_end_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-b6e80c33f8d1>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(file_location, train_start_date, train_end_date, test_start_date, test_end_date, cols_to_use, verbose)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test R2: %.30f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     output_stats_to_csv(file_location,\n\u001b[0m\u001b[0;32m    119\u001b[0m                         \u001b[0mcols_to_use\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                         \u001b[0mmae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-e9b928a6a09c>\u001b[0m in \u001b[0;36moutput_stats_to_csv\u001b[1;34m(file_location, cols_used, mae, mse, rmse, r2, train_start, train_end, test_start, test_end)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         writer.writerow({\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;34m'int_time'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'int_time'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols_used\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;34m'int_date'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'int_date'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols_used\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2544.0_x64__qbz5n2kfra8p0\\lib\\csv.py\u001b[0m in \u001b[0;36mwriterow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2544.0_x64__qbz5n2kfra8p0\\lib\\csv.py\u001b[0m in \u001b[0;36m_dict_to_list\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0mwrong_fields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrong_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 raise ValueError(\"dict contains fields not in fieldnames: \"\n\u001b[0m\u001b[0;32m    150\u001b[0m                                  + \", \".join([repr(x) for x in wrong_fields]))\n\u001b[0;32m    151\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'train_end', 'train_start', 'test_start', 'test_end'"
     ]
    }
   ],
   "source": [
    "# test_powersets(file='./datasets/bss/dublin/reorg_plus_weather/station_50.csv')\n",
    "\n",
    "\n",
    "# Trying to predict 2020 data with older training set\n",
    "test_powersets(\n",
    "               file='./datasets/bss/dublin/reorg_plus_weather/station_4.csv',\n",
    "               train_start_date=datetime(year=2018, month=8, day=1),\n",
    "               train_end_date=datetime(year=2020, month=1, day=31),\n",
    "               test_start_date=datetime(year=2020, month=4, day=1),\n",
    "               test_end_date=datetime(year=2020, month=12, day=1),\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_performance_deltas(file='./datasets/bss/dublin/feature_optimization_stats/station_2.csv'):\n",
    "    attr_list = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "    ]\n",
    "    \n",
    "    stats_list = [\n",
    "        'mae',\n",
    "        'mse',\n",
    "        'rmse',\n",
    "        'r2'\n",
    "    ]\n",
    "    \n",
    "    stats = read_csv(file)\n",
    "    \n",
    "    col_names = ['feature', 'mae', 'mse', 'rmse', 'r2']\n",
    "    destination_directory = './datasets/bss/dublin/feature_optimization_deltas/'\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    destination_file = destination_directory + file.split('/')[-1]\n",
    "\n",
    "    with open(destination_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([col_names])\n",
    "    \n",
    "    with open(destination_file, 'a', newline='') as csvfile:\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=col_names)\n",
    "        \n",
    "        for attr in attr_list:\n",
    "            stats_with_attr = stats[stats[attr] == True]\n",
    "            stats_without_attr = stats[stats[attr] == False]\n",
    "            print(\"--- \" + attr + \" ---\")\n",
    "            line_dict = {}\n",
    "            line_dict['feature'] = attr\n",
    "            for stat in stats_list:\n",
    "                average_with = stats_with_attr[stat].sum() / len(stats_with_attr)\n",
    "                average_without = stats_without_attr[stat].sum() / len(stats_without_attr)\n",
    "                print(stat.upper())\n",
    "                print(\"Average with: \" + str(average_with))\n",
    "                print(\"Average without: \" + str(average_without))\n",
    "                print(\"Average delta (with-without): \" + str(average_with - average_without))\n",
    "                line_dict[stat] = (average_with - average_without)\n",
    "                print()\n",
    "            \n",
    "            writer.writerow(line_dict)\n",
    "        \n",
    "feature_performance_deltas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-darkness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
