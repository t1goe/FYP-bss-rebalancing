{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "northern-effort",
   "metadata": {},
   "source": [
    "# LSTM Feature optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-hotel",
   "metadata": {},
   "source": [
    "The purposes of this is to explore optimization for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import keras\n",
    "from datetime import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_date(df, date):\n",
    "    # print(date)\n",
    "    x = df.index[df['DATE'] == str(date).split(' ')[0]].tolist()\n",
    "    if len(x) == 0:\n",
    "        print(\"Date: \" + str(date) + \" not found in dataset\")\n",
    "        exit(1)\n",
    "\n",
    "    return x[0]\n",
    "\n",
    "\n",
    "def output_stats_to_csv(file_location, cols_used, mae, mse, rmse, r2):\n",
    "    col_names = ['int_time',\n",
    "                 'int_date',\n",
    "                 'int_day',\n",
    "                 'rain',\n",
    "                 'temp',\n",
    "                 'rhum',\n",
    "                 'mae',\n",
    "                 'mse',\n",
    "                 'rmse',\n",
    "                 'r2',\n",
    "                 'rmsle',\n",
    "                 'timestamp'\n",
    "                ]\n",
    "    \n",
    "    destination_directory = './datasets/bss/dublin/feature_optimization_stats/'\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    destination_file = destination_directory + file_location.split('/')[-1]\n",
    "\n",
    "    if not os.path.exists(destination_file):\n",
    "\n",
    "        with open(destination_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows([col_names])\n",
    "\n",
    "    with open(destination_file, 'a', newline='') as csvfile:\n",
    "\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=col_names)\n",
    "\n",
    "        writer.writerow({\n",
    "            'int_time': ('int_time' in cols_used),\n",
    "            'int_date': ('int_date' in cols_used),\n",
    "            'int_day': ('int_day' in cols_used),\n",
    "            'rain': ('rain' in cols_used),\n",
    "            'temp': ('temp' in cols_used),\n",
    "            'rhum': ('rhum' in cols_used),\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'timestamp': datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-president",
   "metadata": {},
   "source": [
    "This is a modified version of the model used to more easily use different combonations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(file_location,\n",
    "                train_start_date=datetime(year=2018, month=8, day=1),\n",
    "                train_end_date=datetime(year=2019, month=7, day=30),\n",
    "                test_start_date=datetime(year=2019, month=8, day=1),\n",
    "                test_end_date=datetime(year=2019, month=12, day=31),\n",
    "                cols_to_use=None,\n",
    "                verbose=1\n",
    "                ):\n",
    "    if cols_to_use is None:\n",
    "        cols_to_use = ['int_time', 'int_date', 'int_day']\n",
    "\n",
    "    cols_to_use.insert(0, 'AVAILABLE BIKES')\n",
    "    cols_to_use.insert(0, 'TIME')\n",
    "    # load dataset\n",
    "    dataset = read_csv(file_location, usecols=cols_to_use)\n",
    "    dataset['DATE'] = dataset['TIME'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "    if 'rain' in cols_to_use:\n",
    "        dataset = dataset[dataset['rain'].str.strip().astype(bool)]\n",
    "\n",
    "    train_start_index = (get_index_of_date(dataset, train_start_date))\n",
    "    train_end_index = (get_index_of_date(dataset, train_end_date))\n",
    "    # print( train_end_index - train_start_index)\n",
    "\n",
    "    test_start_index = (get_index_of_date(dataset, test_start_date))\n",
    "    test_end_index = (get_index_of_date(dataset, test_end_date))\n",
    "    # print(test_end_index - test_start_index)\n",
    "\n",
    "    dataset = dataset.drop(['TIME', 'DATE'], axis=1)\n",
    "    # print(dataset.head())\n",
    "    # print(dataset)\n",
    "    values = dataset.values\n",
    "    # print(values.shape)\n",
    "\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # print(values.shape)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    # frame as supervised learning\n",
    "    reframed = scaled\n",
    "\n",
    "    # print(scaled)\n",
    "\n",
    "    # split into train and test sets\n",
    "    # values = reframed.values\n",
    "\n",
    "    train = scaled[train_start_index:train_end_index, :]\n",
    "    test = scaled[test_start_index:test_end_index, :]\n",
    "    # train = values[train_start:train_end, :]\n",
    "    # test = values[test_start:test_end, :]\n",
    "\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, 1:], train[:, 0]\n",
    "    test_X, test_y = test[:, 1:], test[:, 0]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    # print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y,\n",
    "                        epochs=150,\n",
    "                        batch_size=72,\n",
    "                        validation_data=(test_X, test_y),\n",
    "                        verbose=verbose,\n",
    "                        shuffle=False)\n",
    "    # plot history\n",
    "    # pyplot.plot(history.history['loss'], label='train')\n",
    "    # pyplot.plot(history.history['val_loss'], label='test')\n",
    "    # pyplot.legend()\n",
    "    # pyplot.show()\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    # print(test_X)\n",
    "    # print(yhat)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "\n",
    "    inv_yhat = concatenate((yhat, test_X), axis=1)\n",
    "    # print(yhat.shape)\n",
    "    # print(test_X.shape)\n",
    "    # print(inv_yhat.shape)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:, 0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:, 0]\n",
    "    # calculate RMSE\n",
    "\n",
    "    # np.set_printoptions(threshold=sys.maxsize)\n",
    "    # temp = concatenate((inv_y, inv_yhat))\n",
    "    # print(temp)\n",
    "    # print(inv_y)\n",
    "    # print(inv_yhat)\n",
    "\n",
    "    # print()\n",
    "#      mean_squared_log_error\n",
    "#  mean_absolute_percentage_error\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "    mse = mean_squared_error(inv_y, inv_yhat)\n",
    "    r2 = r2_score(inv_y, inv_yhat)\n",
    "    print('Test MAE: %.3f' % mae)\n",
    "    print('Test MSE: %.3f' % mse)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print('Test R2: %.3f' % r2)\n",
    "\n",
    "    output_stats_to_csv(file_location, cols_to_use, mae, mse, rmse, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "\n",
    "\n",
    "def test_powersets(start_position=0,\n",
    "                   file='./datasets/bss/dublin/reorg_plus_weather/station_2.csv',\n",
    "                   train_start_date=None,\n",
    "                   train_end_date=None,\n",
    "                   test_start_date=None,\n",
    "                   test_end_date=None,\n",
    "                   ):\n",
    "    attr_list = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "    ]\n",
    "\n",
    "    y = list(powerset(attr_list))\n",
    "    # y.sort()\n",
    "    print(len(y))\n",
    "    y = sorted(y, key=len)\n",
    "    y.pop(0)\n",
    "\n",
    "    for x in y[start_position:]:\n",
    "        print(str(start_position) + \"/\" + str(len(y) - 1))\n",
    "        start_position = start_position + 1\n",
    "        print(x)\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        if train_start_date is not None and train_end_date is not None and test_start_date is not None and test_end_date is not None:\n",
    "            train_model(file,\n",
    "                        train_start_date=train_start_date,\n",
    "                        train_end_date=train_end_date,\n",
    "                        test_start_date=test_start_date,\n",
    "                        test_end_date=test_end_date,\n",
    "                        cols_to_use=x,\n",
    "                        verbose=0)\n",
    "        else:\n",
    "            train_model(file,\n",
    "                        cols_to_use=x,\n",
    "                        verbose=0)\n",
    "        print()\n",
    "        keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_powersets()\n",
    "\n",
    "\n",
    "# # Trying to predict 2020 data with older training set\n",
    "# test_powersets(\n",
    "#                file='./datasets/bss/dublin/reorg_plus_weather/station_4.csv',\n",
    "#                train_start_date=datetime(year=2018, month=8, day=1),\n",
    "#                train_end_date=datetime(year=2020, month=1, day=31),\n",
    "#                test_start_date=datetime(year=2020, month=4, day=1),\n",
    "#                test_end_date=datetime(year=2020, month=12, day=1),\n",
    "#                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blind-saturn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- int_time ---\n",
      "MAE\n",
      "Average with: 4.5010455\n",
      "Average without: 4.682699425806451\n",
      "Average delta (with-without): -0.1816539258064509\n",
      "\n",
      "MSE\n",
      "Average with: 31.944110656249997\n",
      "Average without: 34.347313516129034\n",
      "Average delta (with-without): -2.4032028598790376\n",
      "\n",
      "RMSE\n",
      "Average with: 5.631697972911484\n",
      "Average without: 5.847463181328008\n",
      "Average delta (with-without): -0.21576520841652425\n",
      "\n",
      "R2\n",
      "Average with: 0.15569050173107363\n",
      "Average without: 0.09217517731398617\n",
      "Average delta (with-without): 0.06351532441708746\n",
      "\n",
      "--- int_date ---\n",
      "MAE\n",
      "Average with: 4.53674219375\n",
      "Average without: 4.645851225806451\n",
      "Average delta (with-without): -0.10910903205645095\n",
      "\n",
      "MSE\n",
      "Average with: 33.15502790625\n",
      "Average without: 33.09733441935484\n",
      "Average delta (with-without): 0.05769348689516107\n",
      "\n",
      "RMSE\n",
      "Average with: 5.740631393782091\n",
      "Average without: 5.735015779138992\n",
      "Average delta (with-without): 0.0056156146430987874\n",
      "\n",
      "R2\n",
      "Average with: 0.12368474694836191\n",
      "Average without: 0.12521337579936598\n",
      "Average delta (with-without): -0.001528628851004063\n",
      "\n",
      "--- int_day ---\n",
      "MAE\n",
      "Average with: 4.2897557781249995\n",
      "Average without: 4.900804945161291\n",
      "Average delta (with-without): -0.6110491670362919\n",
      "\n",
      "MSE\n",
      "Average with: 28.296781125\n",
      "Average without: 38.112298838709684\n",
      "Average delta (with-without): -9.815517713709685\n",
      "\n",
      "RMSE\n",
      "Average with: 5.316683181800477\n",
      "Average without: 6.172639739894208\n",
      "Average delta (with-without): -0.8559565580937312\n",
      "\n",
      "R2\n",
      "Average with: 0.2520922793181438\n",
      "Average without: -0.0073363350339572244\n",
      "Average delta (with-without): 0.259428614352101\n",
      "\n",
      "--- rain ---\n",
      "MAE\n",
      "Average with: 4.596845075\n",
      "Average without: 4.5838095419354845\n",
      "Average delta (with-without): 0.013035533064515548\n",
      "\n",
      "MSE\n",
      "Average with: 33.274107656249996\n",
      "Average without: 32.974413387096774\n",
      "Average delta (with-without): 0.29969426915322117\n",
      "\n",
      "RMSE\n",
      "Average with: 5.7502926439310915\n",
      "Average without: 5.725042875759381\n",
      "Average delta (with-without): 0.025249768171710762\n",
      "\n",
      "R2\n",
      "Average with: 0.12062849899688932\n",
      "Average without: 0.12836821239443447\n",
      "Average delta (with-without): -0.007739713397545145\n",
      "\n",
      "--- temp ---\n",
      "MAE\n",
      "Average with: 4.619077750000001\n",
      "Average without: 4.5608596838709685\n",
      "Average delta (with-without): 0.05821806612903213\n",
      "\n",
      "MSE\n",
      "Average with: 32.990170468749994\n",
      "Average without: 33.26750983870968\n",
      "Average delta (with-without): -0.2773393699596838\n",
      "\n",
      "RMSE\n",
      "Average with: 5.725730532238111\n",
      "Average without: 5.750397313636006\n",
      "Average delta (with-without): -0.024666781397894688\n",
      "\n",
      "R2\n",
      "Average with: 0.12804205189079151\n",
      "Average without: 0.12071551263298703\n",
      "Average delta (with-without): 0.007326539257804479\n",
      "\n",
      "--- rhum ---\n",
      "MAE\n",
      "Average with: 4.572759440624999\n",
      "Average without: 4.608672132258065\n",
      "Average delta (with-without): -0.035912691633066096\n",
      "\n",
      "MSE\n",
      "Average with: 32.80778209375\n",
      "Average without: 33.455781709677424\n",
      "Average delta (with-without): -0.647999615927425\n",
      "\n",
      "RMSE\n",
      "Average with: 5.71143602144932\n",
      "Average without: 5.76515293767605\n",
      "Average delta (with-without): -0.05371691622673058\n",
      "\n",
      "R2\n",
      "Average with: 0.13286274377015417\n",
      "Average without: 0.11573931456396754\n",
      "Average delta (with-without): 0.017123429206186633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def feature_performance_deltas(file='./datasets/bss/dublin/feature_optimization_stats/station_2.csv'):\n",
    "    attr_list = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "    ]\n",
    "    \n",
    "    stats_list = [\n",
    "        'mae',\n",
    "        'mse',\n",
    "        'rmse',\n",
    "        'r2'\n",
    "    ]\n",
    "    \n",
    "    stats = read_csv(file)\n",
    "    \n",
    "#     col_names = ['feature', 'mae', 'mse', 'rmse', 'r2']\n",
    "    col_names = deepcopy(stats_list)\n",
    "    col_names.insert(0, 'feature')\n",
    "    destination_directory = './datasets/bss/dublin/feature_optimization_deltas/'\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    destination_file = destination_directory + file.split('/')[-1]\n",
    "\n",
    "    with open(destination_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows([col_names])\n",
    "    \n",
    "    with open(destination_file, 'a', newline='') as csvfile:\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=col_names)\n",
    "        \n",
    "        for attr in attr_list:\n",
    "            stats_with_attr = stats[stats[attr] == True]\n",
    "            stats_without_attr = stats[stats[attr] == False]\n",
    "            print(\"--- \" + attr + \" ---\")\n",
    "            line_dict = {}\n",
    "            line_dict['feature'] = attr\n",
    "            for stat in stats_list:\n",
    "                average_with = stats_with_attr[stat].sum() / len(stats_with_attr)\n",
    "                average_without = stats_without_attr[stat].sum() / len(stats_without_attr)\n",
    "                print(stat.upper())\n",
    "                print(\"Average with: \" + str(average_with))\n",
    "                print(\"Average without: \" + str(average_without))\n",
    "                print(\"Average delta (with-without): \" + str(average_with - average_without))\n",
    "                line_dict[stat] = (average_with - average_without)\n",
    "                print()\n",
    "            \n",
    "            writer.writerow(line_dict)\n",
    "        \n",
    "feature_performance_deltas(file='./datasets/bss/dublin/feature_optimization_stats/station_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-darkness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
