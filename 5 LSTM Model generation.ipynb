{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominican-constitution",
   "metadata": {},
   "source": [
    "# LSTM Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contrary-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fluid-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_date(df, date):\n",
    "    # print(date)\n",
    "    x = df.index[df['DATE'] == str(date).split(' ')[0]].tolist()\n",
    "    if len(x) == 0:\n",
    "        print(\"Date: \" + str(date) + \" not found in dataset\")\n",
    "        if date.year <= 2018:\n",
    "            print(\"Assuming before start of dataset, returning 0\")\n",
    "            return 0\n",
    "        elif date.year >= 2020:\n",
    "            print(\"Assuming after end of dataset, returning end\")\n",
    "            return (len(df) - 1)\n",
    "            \n",
    "\n",
    "    return x[0]\n",
    "\n",
    "def get_data_split(\n",
    "                file_location,\n",
    "                train_start_date=datetime(year=2018, month=8, day=1),\n",
    "                train_end_date=datetime(year=2019, month=7, day=30),\n",
    "                test_start_date=datetime(year=2019, month=8, day=1),\n",
    "                test_end_date=datetime(year=2019, month=12, day=31),\n",
    "                cols_to_use=None\n",
    "                ):\n",
    "    if cols_to_use is None:\n",
    "        cols_to_use = [\n",
    "        'int_time',\n",
    "        'int_date',\n",
    "        'int_day',\n",
    "        'rain',\n",
    "        'temp',\n",
    "        'rhum'\n",
    "        ]\n",
    "\n",
    "    cols_to_use.insert(0, 'AVAILABLE BIKES')\n",
    "    cols_to_use.insert(0, 'TIME')\n",
    "    # load dataset\n",
    "    dataset = read_csv(file_location, usecols=cols_to_use)\n",
    "    dataset['DATE'] = dataset['TIME'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "    if 'rain' in cols_to_use:\n",
    "        dataset = dataset[dataset['rain'].str.strip().astype(bool)]\n",
    "\n",
    "    train_start_index = (get_index_of_date(dataset, train_start_date))\n",
    "    train_end_index = (get_index_of_date(dataset, train_end_date))\n",
    "    # print( train_end_index - train_start_index)\n",
    "\n",
    "    test_start_index = (get_index_of_date(dataset, test_start_date))\n",
    "    test_end_index = (get_index_of_date(dataset, test_end_date))\n",
    "    # print(test_end_index - test_start_index)\n",
    "\n",
    "    dataset = dataset.drop(['TIME', 'DATE'], axis=1)\n",
    "    # print(dataset.head())\n",
    "    # print(dataset)\n",
    "    values = dataset.values\n",
    "    # print(values.shape)\n",
    "\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float32')\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # print(values.shape)\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    # frame as supervised learning\n",
    "    reframed = scaled\n",
    "\n",
    "    # print(scaled)\n",
    "\n",
    "    # split into train and test sets\n",
    "    # values = reframed.values\n",
    "\n",
    "    train = scaled[train_start_index:train_end_index, :]\n",
    "    test = scaled[test_start_index:test_end_index, :]\n",
    "    # train = values[train_start:train_end, :]\n",
    "    # test = values[test_start:test_end, :]\n",
    "\n",
    "    # split into input and outputs\n",
    "    train_x, train_y = train[:, 1:], train[:, 0]\n",
    "    test_x, test_y = test[:, 1:], test[:, 0]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "    test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
    "    # print(train_X.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, scaler\n",
    "\n",
    "def get_trained_model(train_x, train_y, test_x, test_y, verbose=1):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_x, train_y,\n",
    "                        epochs=150,\n",
    "                        batch_size=72,\n",
    "                        validation_data=(test_x, test_y),\n",
    "                        verbose=verbose,\n",
    "                        shuffle=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "absent-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_10 station model already exists\n",
      "station_100 station model already exists\n",
      "station_101 station model already exists\n",
      "station_102 station model already exists\n",
      "station_103 station model already exists\n",
      "station_104 station model already exists\n",
      "station_105 station model already exists\n",
      "station_106 station model already exists\n",
      "station_107 station model already exists\n",
      "station_108 station model already exists\n",
      "station_109 station model already exists\n",
      "station_11 station model already exists\n",
      "station_110 station model already exists\n",
      "station_111 station model already exists\n",
      "station_112 station model already exists\n",
      "station_113 station model already exists\n",
      "station_114 station model already exists\n",
      "station_115 station model already exists\n",
      "\n",
      " Working on station_116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thest\\documents\\github\\fyp-bss-rebalancing\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n",
      "Date: 2019-07-30 00:00:00 not found in dataset\n",
      "File station_116.csv is causing IndexError issues lol\n",
      "\n",
      " Working on station_117\n",
      "Date: 2018-08-01 00:00:00 not found in dataset\n",
      "Assuming before start of dataset, returning 0\n",
      "Date: 2019-07-30 00:00:00 not found in dataset\n",
      "File station_117.csv is causing IndexError issues lol\n",
      "station_12 station model already exists\n",
      "station_13 station model already exists\n",
      "station_15 station model already exists\n",
      "station_16 station model already exists\n",
      "station_17 station model already exists\n",
      "station_18 station model already exists\n",
      "station_19 station model already exists\n",
      "station_2 station model already exists\n",
      "station_21 station model already exists\n",
      "station_22 station model already exists\n",
      "station_23 station model already exists\n",
      "station_24 station model already exists\n",
      "station_25 station model already exists\n",
      "station_26 station model already exists\n",
      "station_27 station model already exists\n",
      "station_28 station model already exists\n",
      "station_29 station model already exists\n",
      "station_3 station model already exists\n",
      "station_30 station model already exists\n",
      "station_31 station model already exists\n",
      "station_32 station model already exists\n",
      "station_33 station model already exists\n",
      "station_34 station model already exists\n",
      "station_36 station model already exists\n",
      "station_37 station model already exists\n",
      "station_38 station model already exists\n",
      "station_39 station model already exists\n",
      "station_4 station model already exists\n",
      "station_40 station model already exists\n",
      "station_41 station model already exists\n",
      "station_42 station model already exists\n",
      "\n",
      " Working on station_43\n",
      "File station_43.csv is causing AttributeError issues lol\n",
      "station_44 station model already exists\n",
      "station_45 station model already exists\n",
      "station_47 station model already exists\n",
      "station_48 station model already exists\n",
      "station_49 station model already exists\n",
      "station_5 station model already exists\n",
      "station_50 station model already exists\n",
      "station_51 station model already exists\n",
      "station_52 station model already exists\n",
      "station_53 station model already exists\n",
      "station_54 station model already exists\n",
      "station_55 station model already exists\n",
      "station_56 station model already exists\n",
      "station_57 station model already exists\n",
      "station_58 station model already exists\n",
      "station_59 station model already exists\n",
      "station_6 station model already exists\n",
      "station_61 station model already exists\n",
      "station_62 station model already exists\n",
      "station_63 station model already exists\n",
      "station_64 station model already exists\n",
      "station_65 station model already exists\n",
      "station_66 station model already exists\n",
      "station_67 station model already exists\n",
      "station_68 station model already exists\n",
      "station_69 station model already exists\n",
      "station_7 station model already exists\n",
      "station_71 station model already exists\n",
      "station_72 station model already exists\n",
      "station_73 station model already exists\n",
      "station_74 station model already exists\n",
      "station_75 station model already exists\n",
      "station_76 station model already exists\n",
      "station_77 station model already exists\n",
      "station_78 station model already exists\n",
      "station_79 station model already exists\n",
      "station_8 station model already exists\n",
      "station_80 station model already exists\n",
      "station_81 station model already exists\n",
      "station_82 station model already exists\n",
      "station_83 station model already exists\n",
      "station_84 station model already exists\n",
      "station_85 station model already exists\n",
      "station_86 station model already exists\n",
      "station_87 station model already exists\n",
      "station_88 station model already exists\n",
      "station_89 station model already exists\n",
      "station_9 station model already exists\n",
      "station_90 station model already exists\n",
      "station_91 station model already exists\n",
      "station_92 station model already exists\n",
      "station_93 station model already exists\n",
      "station_94 station model already exists\n",
      "station_95 station model already exists\n",
      "station_96 station model already exists\n",
      "station_97 station model already exists\n",
      "station_98 station model already exists\n",
      "station_99 station model already exists\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "destination_directory = './datasets/bss/dublin/ml_models/'\n",
    "if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "source_directory = './datasets/bss/dublin/reorg_plus_weather/'\n",
    "files = [f for f in listdir(source_directory) if isfile(join(source_directory, f))]\n",
    "# for file in tqdm(files):\n",
    "for file in files:\n",
    "    station = file.split('.')[0]\n",
    "    if os.path.exists(destination_directory + station +'.h5'):\n",
    "        print(station + \" station model already exists\")\n",
    "        continue\n",
    "    print(\"\\n Working on \" + station)\n",
    "    try:\n",
    "        train_x, train_y, test_x, test_y, scaler = get_data_split(source_directory + file)\n",
    "    except IndexError as e:\n",
    "        print(\"File \" + file + \" is causing IndexError issues lol\")\n",
    "        continue\n",
    "    except AttributeError as e:\n",
    "        print(\"File \" + file + \" is causing AttributeError issues lol\")\n",
    "        continue\n",
    "        \n",
    "    model = get_trained_model(train_x, train_y, test_x, test_y, verbose=2)       \n",
    "    model.save(destination_directory + station +'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alpine-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = tf.keras.models.load_model(destination_directory + 'station_2.h5')\n",
    "\n",
    "# model = create_model()\n",
    "# model.load_weights(destination_directory + 'station_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worthy-creek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 12.315\n",
      "Test MSE: 232.149\n",
      "Test RMSE: 15.236\n",
      "Test R2: -1.265181346656076399881385441404\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_x)\n",
    "test_x_reshaped = test_x.reshape((test_x.shape[0], test_x.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_x_reshaped), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:, 0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_x_reshaped), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:, 0]\n",
    "# calculate RMSE\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# temp = concatenate((inv_y, inv_yhat))\n",
    "# print(temp)\n",
    "# print(inv_y)\n",
    "# print(inv_yhat)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "mse = mean_squared_error(inv_y, inv_yhat)\n",
    "r2 = r2_score(inv_y, inv_yhat)\n",
    "print('Test MAE: %.3f' % mae)\n",
    "print('Test MSE: %.3f' % mse)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test R2: %.30f' % r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-trail",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
