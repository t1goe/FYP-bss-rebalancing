{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-volunteer",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-pacific",
   "metadata": {},
   "source": [
    "While the dataset is easily accessable from smartdublin and met eireann's websites, the data must be correctly preprocessed before use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-tuner",
   "metadata": {},
   "source": [
    "Imports for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "happy-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "import wget\n",
    "from pandas.errors import DtypeWarning\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-transformation",
   "metadata": {},
   "source": [
    "Downloads the data from their respective websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dublin_weather():\n",
    "    \"\"\"\n",
    "    Download dublin weather data from met eireann\n",
    "    \"\"\"\n",
    "\n",
    "    destination = './datasets/weather'\n",
    "    url = \"https://cli.fusio.net/cli/climate_data/webdata/hly175.zip\"\n",
    "\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    # Download if file does not exist\n",
    "    # Downloads as zip, so code exists to unzip and remove it\n",
    "    if not os.path.isfile(destination + \"/hly175/hly175.csv\"):\n",
    "        print(\"Downloading hourly Dublin weather data\")\n",
    "        wget.download(url, destination)\n",
    "        with zipfile.ZipFile(destination + \"/hly175.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(destination + \"/hly175\")\n",
    "        os.remove(destination + \"/hly175.zip\")\n",
    "    else:\n",
    "        print(\"Dublin weather data already exists\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "dublin_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dublin_bss():\n",
    "    \"\"\"\n",
    "    Download dublin bss data with wget\n",
    "    \"\"\"\n",
    "    destination = './datasets/bss/dublin'\n",
    "\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    base_url = \"https://data.smartdublin.ie/dataset/33ec9fe2-4957-4e9a-ab55-c5e917c7a9ab/resource\"\n",
    "    urls = [\"/99a35442-6878-4c2d-8dff-ec43e91d21d7/download/dublinbikes_20200701_20201001.csv\",\n",
    "            \"/8ddaeac6-4caf-4289-9835-cf588d0b69e5/download/dublinbikes_20200401_20200701.csv\",\n",
    "            \"/aab12e7d-547f-463a-86b1-e22002884587/download/dublinbikes_20200101_20200401.csv\",\n",
    "            \"/5d23332e-4f49-4c41-b6a0-bffb77b33d64/download/dublinbikes_20191001_20200101.csv\",\n",
    "            \"/305d39ac-b6a0-4216-a535-0ae2ddf59819/download/dublinbikes_20190701_20191001.csv\",\n",
    "            \"/76fdda3d-d8be-441b-92dd-0ee36d9c5316/download/dublinbikes_20190401_20190701.csv\",\n",
    "            \"/538165d7-535e-4e1d-909a-1c1bfae901c5/download/dublinbikes_20190101_20190401.csv\",\n",
    "            \"/67ea095f-67ad-47f5-b8f7-044743043848/download/dublinbikes_20181001_20190101.csv\",\n",
    "            \"/9496fac5-e4d7-4ae9-a49a-217c7c4e83d9/download/dublinbikes_20180701_20181001.csv\",\n",
    "            \"/2dec86ed-76ed-47a3-ae28-646db5c5b965/download/dublin.csv\"]\n",
    "\n",
    "    print(\"Downloading dublinbikes data\")\n",
    "    for url in tqdm(urls):\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        if filename in os.listdir(destination):\n",
    "            # print(\"File \\\"\" + filename + \"\\\" already exists\")\n",
    "            continue\n",
    "\n",
    "        final_url = base_url + url\n",
    "        # print(\"starting download on \" + filename)\n",
    "        wget.download(final_url, destination)\n",
    "        # print(filename + \" : downloaded\\n\")\n",
    "\n",
    "    print(\"Finished downloading Dublin BSS data\")\n",
    "\n",
    "    # Clear generated tmp files\n",
    "    for file in os.listdir():\n",
    "        if file[-4:] == \".tmp\":\n",
    "            os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "dublin_bss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-chile",
   "metadata": {},
   "source": [
    "This function reorganizes all the data into CSVs sorted by station instead of yearly quarter. This is to make the data easier to work with, as the size of the dataset means that it is easiet to process the data on a station-by-station basis. This part also removes columns that are either redundant (latitude/longitude) or do not add anything to the analysis (\"LAST UPDATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organise_by_station():\n",
    "    \"\"\"\n",
    "    Reorganise all the dublinbikes CSVs by station instead of by quarter\n",
    "    \"\"\"\n",
    "    # Get list of data files\n",
    "    bss_files = os.listdir('./datasets/bss/dublin')\n",
    "    if 'dublin.csv' in bss_files:\n",
    "        bss_files.remove('dublin.csv')\n",
    "\n",
    "    if 'reorg' in bss_files:\n",
    "        bss_files.remove('reorg')\n",
    "\n",
    "    # Get all the station IDs\n",
    "    dataset = pd.read_csv('./datasets/bss/dublin/dublin.csv',\n",
    "                          usecols=['Number'])\n",
    "    station_ids = []\n",
    "    for d in dataset['Number'].unique():\n",
    "        station_ids.append(d)\n",
    "    station_ids.sort()\n",
    "\n",
    "    # Get column names\n",
    "    columns = pd.read_csv('./datasets/bss/dublin/dublinbikes_20200701_20201001.csv', nrows=1).columns\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    destination = './datasets/bss/dublin/reorg'\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "\n",
    "    print(\"Starting dublinbikes reorganisation\")\n",
    "    for station in station_ids:\n",
    "        if os.path.exists('./datasets/bss/dublin/reorg/station_' + str(station) + '.csv'):\n",
    "            # print('\\tStation CSV already exists')\n",
    "            continue\n",
    "\n",
    "        # print('Working on station: ' + str(station) + loading_wheel[i], end='\\r')\n",
    "        print('Working on station: ' + str(station))\n",
    "        df1 = pd.DataFrame(columns=columns)\n",
    "        for file in bss_files:\n",
    "            df2 = pd.read_csv('./datasets/bss/dublin/' + str(file))\n",
    "            df2 = df2[df2['STATION ID'] == station]\n",
    "            temp = [df1, df2]\n",
    "            df1 = pd.concat(temp)\n",
    "        df1 = df1.drop(df1.columns[[0]], axis=1)\n",
    "        df1 = df1.drop(['LAST UPDATED', 'NAME', 'STATUS', 'ADDRESS', 'LATITUDE', 'LONGITUDE'], axis=1)\n",
    "\n",
    "        df1.to_csv('./datasets/bss/dublin/reorg/station_' + str(station) + '.csv', index=False)\n",
    "\n",
    "    print('\\nFinished dublinbikes reorganisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "organise_by_station()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-light",
   "metadata": {},
   "source": [
    "The first 15 rows in the downloaded CSV contains information describing the dataset, this is removed before processing to allow us to use the dataset with pandas. Much like the station data I also remove some columns that have no use for the project (like msl which is\"Mean Sea Level Pressure\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather_data():\n",
    "    # Remove first n rows, that contain data from before the start of the BSS data\n",
    "    with open('./datasets/weather/hly175/hly175.csv', 'r') as fin:\n",
    "        data = fin.read().splitlines(True)\n",
    "    with open('./datasets/weather/hly175/hly175clean.csv', 'w') as fout:\n",
    "        fout.writelines(data[15])\n",
    "        fout.writelines(data[131152:])\n",
    "\n",
    "    # Remove all rows with non-useful data\n",
    "    warnings.filterwarnings(\"ignore\", category=DtypeWarning)\n",
    "    dataset = pd.read_csv('./datasets/weather/hly175/hly175clean.csv',\n",
    "                          usecols=['date', 'rain', 'temp', 'rhum'])\n",
    "\n",
    "    #Was used for an old approach for joining weather and station data\n",
    "    #dataset[\"epoch\"] = dataset[\"date\"].apply(lambda x: int(time.mktime(time.strptime(x, \"%d-%b-%Y %H:%M\"))))\n",
    "\n",
    "    dataset.to_csv('./datasets/weather/hly175/hly175clean.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_weather_data(station_file):\n",
    "    station_dataframe = pd.read_csv(station_file)\n",
    "    \n",
    "    if 'TIME' not in station_dataframe.columns:\n",
    "        print('TIME column not present')\n",
    "        return\n",
    "\n",
    "    if not os.path.exists('datasets/bss/dublin/reorg_plus_weather'):\n",
    "        os.makedirs('datasets/bss/dublin/reorg_plus_weather')\n",
    "\n",
    "    weather = pd.read_csv('datasets/weather/hly175/hly175clean.csv')\n",
    "\n",
    "    new_weather_dataframe = pd.DataFrame(np.repeat(weather.values, 12, axis=0), columns=weather.columns)\n",
    "\n",
    "    result = pd.concat([station_dataframe, new_weather_dataframe], axis=1)\n",
    "    result.to_csv(r'datasets/bss/dublin/reorg_plus_weather/station_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
